# USED: UNIVERSAL SPEAKER EXTRACTION AND DIARIZATION

## Introduction

multi-talker scenario에서 인간은 speech signal을 누가 말했는지(speaker extraction)의 정보를 바탕으로 언제 말했는지(diarization) 까지를 동시에 파악한다

speaker extraction은 highly overlapped speech 환경에서 최적화 되고, diarization은 mostly sparsely overlapped speech에서 연구된다

이 논문에서는 Universal Speaker Extraction and Diarization framework (USED)를 제안했고 highly overlapped과 sparsely overlapped된 speech를 효과적으로 다룰것을 기대한다
 
1. 현재 speaker extraction 모델에서 모든 speaker의 waveform을 동시에 추출하도록 하였다
2. Scenario-Aware Differentiated Loss (SAD)를 적용하여 다른 두개의 task에 적합하도록 objective 함수를 구현하였다
3. multi-task learning을 사용하기 때문에 diarization의 결과를 이용하여 background noise를 억제하고 target speech의 성능을 향상시켰다

speaker extraction에서 time domain으로 classify하는 것은 frequency domain에서 발생하는 위상 계산(phase estimation) 문제를 피할 수 있다

최근 speaker extraction system은 target speaker가 없을 경우 speech power를 최소화하고, target speaker가 있을 경우 signal-to-distortion(SDR) ratio 또는 scale-invariant(SI) SDR을 최대화하는 방향으로 진행되고 있다

하지만 이러한 방법은 독립적인 2개의 objectives function을 최적화 하는 방향으로 진행되므로 USED에서는 multi-task module을 활용하여 해결하고 있다

speech separation은 모든 speaker의 speech에 대하여 separate하는 것을 추구하고, speaker extraction은 그 시간에 오직 1명의 target speaker를 추출한다

또한 기존 speaker diarization system과 speech separation system의 공통 학습은 많이 연구되었지만, speaker extraction와 연구가 많이 없기 때문에 거기에 초점을 두었다

## Proposed method

- USED(Universal Speaker Extraction and Diarization) model: extraction module과 diarization module간 상호작용을 하는 구조
  - ![image](https://github.com/kimho1wq/TIL/assets/15611500/b9293761-4de4-4240-8bfb-b0c6bf096bd3)
  - $ES_1, \cdots$는 특정(single) $i$ speaker에 대한 speech를 출력한다
  - $SD_1, \cdots$는 t-time에서 모든 speaker에 대한 diarization output으로, 여기서 permutation problem은 발생하지 않는다
  - $Encoder$는 $SE_1, \cdots$의 speaker embedding을 출력하기 위한 CNN-ReLU encoder이다
  - $R_1, \cdots$는 extraction 동안의 speech representations을 의미한다
  - $M_1, \cdots$는 예측된 speech extraciton mask

- Speaker model
  - 기존 $SpEx+$논문에서 제안했던 time domain speaker extraction network를 사용하여 모든 speaker에 대한 speaker embedding을 출력한다
  - 출력된 reference speaker embedding은 각각 mixture speech의 $Encoder$ output과 concatenate되어 다음 step을 진행한다
  - extraction과 diarization과 별개로, $speaker encoder$는 일치하는 각 화자를 예측하는 cross-entropy $L_{spk}$ loss로 학습이 진행된다 


- Extraction module
  - module structure
    - 각 speaker의 representation $R_i$를 얻기위해 D temporal convolutional network(TCN) layer를 사용하였다
    - $R_1, \cdots, R_i$는 concatenate되어 다른 D TCN layers에 forward되며, 각 TCN layer는 B TCN block들을 포함하고 있다
    - 그 후, 1-D CNN과 ReLU로 구성된 mask module에 feedforward되어 $M_i$의 mask를 생성하고, mixture speech의 representation인 $e$와 특정 speaker에 대한 mask를 element-wise multiplication 하여 modulated response를 구한다
    - 마지막으로 tranposed conv-layer인 $decoder$에 입력으로 들어가 각 speaker에 대한 time domain signal인 $ES_i$를 생성한다
  - scenario-aware differentiated(SAD) loss
    - sparsely overlapped speech를 다루기 위해서 SAD framework를 사용했으며, scenario는 QQ, QS, SS, SQ의 4개의 시나리오로 구분되고 Q는 $quite$를 의미하고 S는 $speaking$을 의미하며 앞에서 부터 target speaker와 interference의 state를 의미한다 
      - QQ는 target speaker와 interference speaker가 quiet state이고, SS는 target speaker와 interference speaker가 speaking status이다 
      - QS는 target speaker와 interference가 각각 quiet, speaking status이고, SQ는 target speaker와 interference가 각각 speaking, quiet status이다 
    - QS, QQ의 objective
      - $L_{E_i} = 10\log_{10}{(\frac{||ES_i||^2}{T_{se}} + \epsilon)}$
      - $T_{se}$는 $ES_i$의 seconds(duration)이고, $\epsilon$은 $10^{-6}$이다
    - SS, SQ의 objective
      - $L_{S_i} = -10\log_{10}{\frac{||\frac{<ES_i,s_i>s_i}{||s_i||^2+\epsilon}||^2}{||ES_i - \frac{<ES_i,s_i>s_i}{||s_i||^2+\epsilon} ||^2 + \epsilon}} + \epsilon$
      - $s_{i}$는 target speech이다
    - speaker $i$에 대한 total speaker extraction loss
      - $L_{ext_i} = \alpha L_{E_i}^{QQ} + \beta L_{E_i}^{QS} + \gamma L_{S_i}^{SS + \delta L_{S_i}^{SQ}$
      - $\alpha, \beta, \gamma, \delta$는 weights이다

- Diarization module
  - extraction와 diarization은 서로 다른 time resolution이 필요하기 때문에, 마지막 TCNs block의 output은 $Diarization Encoderd$에 의해서 down sampling된다
  - linear layer에서 target speaker의 speech activites를 생성하고, BCE(binary cross entropy) loss를 사용한다
    - $L_{diar_i} = \frac{1}{T_{sd}}\displaystyle\sum_{t=1}^{T_{sd}}{BCE(y_i^t, SD^t_i)}$
    - $T_{sd}$는 diarization output length이고, $y^t_i$는 t-time에서 speaker $i$에 대한 ground-truth label이다

- Multi-Task interaction
  - diarization의 ouput probabilities에 보간(interpolation) 기술을 적용하여 extraction와 계산을 위해 일관성?을 맞춰주었다
  - 그 후 ouput에 CNN과 ReLU activation을 적용하여 각 speaker에 대한 $r$ value를 얻으며, 이 단계에서 output은 1보다 작도록 제한되지 않는다
  - 마지막으로 $r$ value를 이용하여 extracted speech인 $ES_i$를 generate한다
    - $L = \displaystyle\sum_i^n{\lambda_1 L_{ext} + \lambda_2 L_{diar_i} + L_{spk_i}}$
    - $\lambda_1, \lambda_2$는 loss weights이고, $n$은 maximum speaker의 수 이다

- Speaker shuffling
  - training process에서 data overfitting을 피하기 위해 TS-VAD에서 사용했던 speaker oder shuffling 방법을 약간 다르게 사용하였다
  - 우선 utterances를 chunks로 나누고 이미 chunk안에 존재하는 speakers들을 그것과 일치하는 reference speeches로 사용하였다
  - 그리고 그 reference speeches에 대하여 threshold를 사용하여 그들의 inclusion을 결정하였다
  - 만약 선택된 probability가 threshold를 넘어가면 전체 dataset에서 random하게 speaker를 선택하고, speaker의 순서와 그에 따른 output를 shuffle하였다
  - mixture에서 발생하는 speaker absent와 speaker order에 덜 민감하게 하기 위해서 
  - inference에서는 speaker가 utternace에 존재하지 않는다면 empty embedding을 대신하여 input으로 사용하였다



## Experiments

- Dataset
  - eval은 LibriMix(generated by LibriSpeech train-clean-100 and test-clean, WHAM!)와 SparseLibiriMix를 사용하였다
  - LibiriMix는 speech separation을 위한 db이므로, speaker extraction의 호환성(compatibility)를 위해 약간 수정하였다
  - Libiri2Mix와 Libiri3Mix를 섞어서 사용했으며, min mode(주로 highly overlapped speech)와 max mode(주로 non-overlapped speech)를 둘다 평가하였다
- Baseline configuration
  - TS-VAD를 사용할 때 bidirectional LSTM 대신 4개의 transformer layer를 사용했으며, transformer의 embedding은 384차원이고, attention head는 4개 이다
  - mixture speech에 대하여 time-level speaker embedding을 추출하는 pre-trained speaker encoder(SpEx+)를 사용하였고, SpEx+에서 model의 B=8, D=6으로 설정하였다
- USED configuration
  - extraciton module에서는 decoder의 kernel size는 256으로 고정되고 encoder의 kernel size(stride=10)를 20, 80, 160으로 여러가지를 사용하는 multi-scale structure를 사용했다
  - encoder의 structure는 SpEx+와 동일하고 speaker embedding는 256차원이다
  - TCN에서는 B=8, D=3으로 설정하고, CNN은 kernel size=1, stride=1이며 ReLU activation을 사용하였다
  - diarization module에서는 kernel size 32, stride 16의 CNN layer를 사용하였고, interpolate 후의 CNN layer는 kernel size 160, ReLU를 사용하였다
  - Training 동안 $\alpha, \beta$는 0.001 $\gamma, \delta, \lambda_1, \lambda_2$는 1.0으로 설정하였다
  - utterances는 4 seconds window size, 2 seconds의 shift로 chunk되었다
- Evaluation metrics
  - diarization performance
    - DER(diarization error rate)
    - FA(false alarm)
    - MS(missed detection)
    - SC(speaker confusion)
    - Collar tolerance와 median filtering은 0 sec, 11 frames로 설정되었다
  - speaker extraction performance
    - SI-SDRi(scale invariant SDR improvement)
    - SDRi(SDR improvement)
    - STOI(short-time objective intelligibility)
    - PESQ(perceptual evaluation of speech quality)
      - 음성 품질의 지각 평가로, 대역폭이 제한된 통신 네트워크에서 통신(전화 시스템) 사용자가 경험하는 음성 품질의 평가를 위한 주관적(subjective) 테스트 방법론
      - 주로 심리 음향 모델링을 기반으로하는 알고리즘을 이용하여 계산한다



