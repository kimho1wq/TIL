# EEND-SS: JOINT END-TO-END NEURAL SPEAKER DIARIZATION AND SPEECH SEPARATION FOR FLEXIBLE NUMBER OF SPEAKERS

### Introduction

화자 분리(speaker diarization)와 음성 분리(speech separation)은 다양한 speech processing application에서 중요한 기술이다

speaker diarization은 task는 어떠한 음성을 누가 언제 말했냐(who spoke when)는 것을 찾아내는 것이다

speech separation은 mixture된 input audio에서 각각의 화자를 분리(separation)하는 것이다

diarization의 정보를 안다면 separation에 도움이 되고 그 반대도 마찬가지(vice versa) 이지만 대부분의 경우 다른 정보를 미리 알수가 없다


기존 clustering based system 에서는 동시에 여러 화자가 같은 시간에 말하지 않는 다는 것을 가정하므로 화자가 overlap된 데이터를 다룰 수 없고, end-to-end 모델 또한 아니다

초기의 end-to-end neural diarization(EEND)의 시스템은 overlap된 학습 데이터를 이용하여 훈련이 가능하지만 speaker의 수가 고정되어 있어야 한다

EEND-EDA는 EEND 모델에 LSTM encoder-decoder based attractor calculation을 사용하여 speaker의 수를 subtask로 count한다(overlap handling 가능)

EEND-SS(speaker diarization and separation) 시스템은 speech separation와 diarization를 서로의 성능을 더 좋게 하기위해 multi-task learning을 이용하여  하나의 네트워크에서 학습시켰다 

EEND-SS는 separation model로는 Conv-TasNet, diarization model로는 EEND-EDA를 사용한다

EEND-SS는 Online Recurrent Selective Attention Network(RSAN)에 유사하게 speech separation, speaker diarization, speaker counting의 3가지 error를 동시에 학습시킨다

또한 성능을 향상시키기 위해 2가지 방법을 추가로 적용했는데
여러명의 화자가 섞인 audio를 분리하기 위한 separation masks로 multiple 1x1 convolutional layer를 사용하며
speech activity로 학습된 diarization branch에서 speech를 separate하는 방법으로 사용한다


### Conventional methods

- conventional tasks
  - anechoic환경에서 C speaker의 single-chanel이고 T길이의 mixture speech를 $x \in R^{1\*T}$라 하면, $x = \displaystyle\sum_{c=1}^C{y_c s_c + n}$으로 표현할 수 있다
    - $s_c \in R^{1\*T}$는 c화자의 source speech signal이고, $y_c \in (0,1)$는 c화자의 speech activity, $n \in R^{1\*T}$는 noise signal이다
    - $y_{c,t} = 1$이면, $c$ speaker가 $t$ time에 말하고 있다는 의미이고,  $y_{c,t} = 0$는 $c$ speaker의 silence 상황이다
  - 화자 분리(speaker diarization)는 spekaer label $\hat{Y} = \hat{y}_{c,t} \in (0,1)^{C\*T}$를 계산한다
  - 음성 분리(speech separation)는 분리될 speech signal $\hat{s}_1, \cdots, \hat{s}_C \in R^{1\*T}$를 예측한다
  - 화자 카운팅(speaker counting)은 $x$가 주어졌을 때 화자의 수 $\hat{C}$를 계산한다


### Proposed method
![image](https://github.com/kimho1wq/TIL/assets/15611500/dab20095-6c35-4ce0-9aae-07a1fe698d38)

- joint end-to-end neural speaker diarization and separation (EEND-SS) 시스템은 3개의 task(speaker diarization, speech separation, speaker counting)를 동시에 수행한다

- speaker diarization
  - input mixture $x$에 대하여 feature extraction을 수행하여 log-mel filterbank 특징을 추출한다
  - temporal convolutional network(TCN)s는 $B$ 차원 embedding $e_t^{tcn} \in R^B$를 출력하고, $e_t^{tcn}$은 TCN bottleneck features 라고 한다
  - log-mel filterbank(LMF) features와 TCN bottleneck features를 concatenate하여 EEND-SS diarization에 입력으로 넣는다
    - $(e_t)^T_{t=1} = TrasformerEncoder(Concat((e_t^{tcn})_{t=1}^T, LogMelFilterbank(x)))$
- speaker counting
  - EEND-SS에는 $C_{max}$의 maximum possible speaker number를 사용하고 있다
  - 기존 EEND-SS에 mask의 일반화 성능을 높이기 위해서 multiple $1\*1$ convolutional layer를 사용했다
- speech separation  
  - 기존 Conv-TasNet의 separation 모듈의 마지막 $1\*1 conv$ 에서는 mixture된 speech를 고정된 크기의 $C$명의 speaker로 가정하여 문제를 해결하고 있다
  - EEND-SS에서 사용하는 $C_{max}$의 maximum possible speaker number를 이용하여 기존 Conv-TasNet의 mask vector를 구하는 방식을 변경했다
    - $(m_{c,t})^k_{c=1} = \sigma(1\*1Conv_k(PReLU(e^{TCN}_t)))$
    - ![image](https://github.com/kimho1wq/TIL/assets/15611500/1a686225-2c34-4ac3-8181-952aba33481a)
  - 총 $C_{max}$개의  $1\*1 conv$ (mask)를 학습시키고, $k$번째 $1\*1 conv$는 $k$명의 speaker에 대한 mask를 학습하게 된다
  - training에서는 oracle number of $C$ speaker를 사용하고 $(k=C)$, inference에서는 diarization branch에서 계산된 $\hat{C}$를 사용한다 $(k=\hat{C})$
  - $C_{max}$개의 $1\*1 conv$가 존재하지만 1개의 layer만 speaker number로 사용되며, 성능과 화자의 수를 고려하여 충분히 큰 $C_{max}$를 사용하였다
  - 기존에 VAD(voice activity detection)과 separated speech signal을 함께 적용했을 때 성능이 향상되는 실험이 존재했다
  - 따라서 EEND-SS에서 출력되는 speaker activity $\hat{s}_c = Decoder(d_c)$에 diarization branch에서 출력되는 speech activity probabilities $p$를 사용하여 speaker가 존재하지 않은 곳의 background noise를 감소시킨다
    - $\hat{s}^\prime = \hat{s} \odot p^{\phi_{max}}$
    - $p^{\phi_{max}} := argmax_{(\phi_1, \cdots, \phi_C) \in \Phi(C)}\displaystyle\sum_{c=1}^C{r(abs(\hat{s}), p^\phi))}$
    - $r(\cdot, \cdot)$은 correlation function, $\Phi(C)$는 $(1,\cdots, C)$의 가능한 permutation, $p^\phi := [p_{\phi_{c,t}} \in (0,1) | c = 1,cdots,C]$는 permuted 사후 확률(posterior probabilities)

- training
  - $L = \lambda_1 L_{SI-SDR} + \lambda_2 L_{diar} + \lambda_3 L_{exist}$의 multi-task cost function을 사용하여 학습시킨다
  - $\lambda_1, \lambda_2, \lambda_3 \in R_{+}$는 weighting parameter로 각각 $1.0, 0.2, 0.2$이며 모두 empirically하게 결정하였다

- inference
  - 다양한 수의 speaker를 다루기 위해 2개의 procedure로 진행된다
  - 1.diarization probabilities $p_t$와 speaker의 수 $\hat{C}$를 얻는다
  - 2.speaker의 수 $\hat{C}$에 따른 $1\*1 conv$ layer가 결정되고, separated speech signals $\hat{s}\_1, \cdots, \hat{s}\_{\hat{C}}$를 얻는다
  - 3.추가적으로 $p_t$를 사용하여 $\hat{s}$를 $\hat{s}^\prime = \hat{s} \odot p_t$로 refining한다




### Experiments 

#### Experimental settings
- Dataset
  - LibriMix, SparseLibriMix datasets은 noisy 환경에서의 source separation를 위한 데이터로 다양한 overlap 비율을 사용하며 open source 데이터셋이다
  - LibriMix와 SparseLibriMix는 LibriSpeech로 부터 가져온 train-clean100, dev-clean, test-clean 데이터와 WHAM!(Wsj0 Hipster Ambient Mixtures)으로 부터 가져온 noise samples을 mixture하여 trianing/validation/testing 데이터를 만들었다
  - dataset은 training/validation/test 순으로 two-speaker mixture(Libri2Mix)의 58h/11h/11h sets, three-speaker mixture(Libri3Mix)의 40h/11h/11h sets 이다
  - 8kHz sampling rate와 utterance를 mixture할 때 짧은 utterance에 대한 길이만큼 mixture하는 $min$ 모드를 사용했다
- Configurations
  - 80차원 log-mel filterbank를 사용했으며 frame length는 512 samples, frame shift는 64 samples로 power spectra로 계산하였다
  - Conv-TasNet, EEND-EDA, EEND-SS는 기존 model parameters를 그대로 사용한다
- Evaluation metrics
  - source-to-distortion ratio improvement (SDR(dB))
  - scale-invariant source-to-distortion ratio improvement (SI-SDR(dB))
  - short-time objective intelligibility (STOI)
  - diarization error rate (DER(%))
  - speaker counting accuracy (SCA(%))

  
#### Results

speaker가 active하지 않은 곳을 예측하는 speech activity의 값을 speech separate에 적용한 결과 separation 성능이 향상되었다

log-mel filterbank 보다 HuBERT나 wav2vec을 이용한 speech processing의 feature를 사용하는 것이 더 좋은 성능을 보였다

speaker $C$의 개수를 가변적으로 만들었기 때문에 separation performance를 측정하기 위해 separated speech signal $\hat{C}$와 reference speech signal $C$의 수를 맞춰줘야 한다

따라서 $|C-\hat{C}|$의 silent(amplitude: $10^{-6}$) audio signal을 더 적은 speech에 append하는 방식을 사용했다

separate와 diarize를 동시에 수행하는 모델은 overlapping의 percent가 mismatch된 scenario에서 잘 동작한다










