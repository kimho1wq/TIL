# EEND-SS: JOINT END-TO-END NEURAL SPEAKER DIARIZATION AND SPEECH SEPARATION FOR FLEXIBLE NUMBER OF SPEAKERS

### Introduction

화자 분리(speaker diarization)와 음성 분리(speech separation)은 다양한 speech processing application에서 중요한 기술이다

speaker diarization은 task는 어떠한 음성을 누가 언제 말했냐(who spoke when)는 것을 찾아내는 것이다

speech separation은 mixture된 input audio에서 각각의 화자를 분리(separation)하는 것이다

diarization의 정보를 안다면 separation에 도움이 되고 그 반대도 마찬가지(vice versa) 이지만 대부분의 경우 다른 정보를 미리 알수가 없다


기존 clustering based system 에서는 동시에 여러 화자가 같은 시간에 말하지 않는 다는 것을 가정하므로 화자가 overlap된 데이터를 다룰 수 없고, end-to-end 모델 또한 아니다

초기의 end-to-end neural diarization(EEND)의 시스템은 overlap된 학습 데이터를 이용하여 훈련이 가능하지만 speaker의 수가 고정되어 있어야 한다

EEND-EDA는 EEND 모델에 LSTM encoder-decoder based attractor calculation을 사용하여 speaker의 수를 subtask로 count한다(overlap handling 가능)

EEND-SS(speaker diarization and separation) 시스템은 speech separation와 diarization를 서로의 성능을 더 좋게 하기위해 multi-task learning을 이용하여  하나의 네트워크에서 학습시켰다 

EEND-SS는 separation model로는 Conv-TasNet, diarization model로는 EEND-EDA를 사용한다

EEND-SS는 Online Recurrent Selective Attention Network(RSAN)에 유사하게 speech separation, speaker diarization, speaker counting의 3가지 error를 동시에 학습시킨다

또한 성능을 향상시키기 위해 2가지 방법을 추가로 적용했는데
여러명의 화자가 섞인 audio를 분리하기 위한 separation masks로 multiple 1x1 convolutional layer를 사용하며
speech activity로 학습된 diarization branch에서 speech를 separate하는 방법으로 사용한다


### Conventional methods

- conventional tasks
  - anechoic환경에서 C speaker의 single-chanel이고 T길이의 mixture speech를 $x \in R^{1\*T}$라 하면, $x = \displaystyle\sum_{c=1}^C{y_c s_c + n}$으로 표현할 수 있다
    - $s_c \in R^{1\*T}$는 c화자의 source speech signal이고, $y_c \in (0,1)$는 c화자의 speech activity, $n \in R^{1\*T}$는 noise signal이다
    - $y_{c,t} = 1$이면, $c$ speaker가 $t$ time에 말하고 있다는 의미이고,  $y_{c,t} = 0$는 $c$ speaker의 silence 상황이다
  - 화자 분리(speaker diarization)는 spekaer label $\hat{Y} = \hat{y}_{c,t} \in (0,1)^{C\*T}$를 계산한다
  - 음성 분리(speech separation)는 분리될 speech signal $\hat{s}_1, \cdots, \hat{s}_C \in R^{1\*T}$를 예측한다
  - 화자 카운팅(speaker counting)은 $x$가 주어졌을 때 화자의 수 $\hat{C}$를 계산한다


### Proposed method
![image](https://github.com/kimho1wq/TIL/assets/15611500/dab20095-6c35-4ce0-9aae-07a1fe698d38)

- joint end-to-end neural speaker diarization and separation (EEND-SS) 시스템은 3개의 task(speaker diarization, speech separation, speaker counting)를 동시에 수행한다

- speaker diarization
  - input mixture $x$에 대하여 feature extraction을 수행하여 log-mel filterbank 특징을 추출한다
  - temporal convolutional network(TCN)s는 $B$ 차원 embedding $e_t^{tcn} \in R^B$를 출력하고, $e_t^{tcn}$은 TCN bottleneck features 라고 한다
  - log-mel filterbank(LMF) features와 TCN bottleneck features를 concatenate하여 EEND-SS diarization에 입력으로 넣는다
    - $(e_t)^T_{t=1} = TrasformerEncoder(Concat((e_t^{tcn})_{t=1}^T, LogMelFilterbank(x)))$
- speaker counting
  - EEND-SS에는 $C_{max}$의 maximum possible speaker number를 사용하고 있다
  - 기존 EEND-SS에 mask의 일반화 성능을 높이기 위해서 multiple $1\*1$ convolutional layer를 사용했다
- speech separation  
  - 기존 Conv-TasNet의 separation 모듈의 마지막 $1\*1 conv$ 에서는 mixture된 speech를 고정된 크기의 $C$명의 speaker로 가정하여 문제를 해결하고 있다
  - EEND-SS에서 사용하는 $C_{max}$의 maximum possible speaker number를 이용하여 기존 Conv-TasNet의 mask vector를 구하는 방식을 변경했다
    - $(m_{c,t})^k_{c=1} = \sigma(1\*1Conv_k(PReLU(e^{TCN}_t)))$
    - ![image](https://github.com/kimho1wq/TIL/assets/15611500/1a686225-2c34-4ac3-8181-952aba33481a)
  - 총 $C_{max}$개의  $1\*1 conv$ (mask)를 학습시키고, $k$번째 $1\*1 conv$는 $k$명의 speaker에 대한 mask를 학습하게 된다
  - training에서는 oracle number of $C$ speaker를 사용하고 $(k=C)$, inference에서는 diarization branch에서 계산된 $\hat{C}$를 사용한다 $(k=\hat{C})$
  - $C_{max}$개의 $1\*1 conv$가 존재하지만 1개의 layer만 speaker number로 사용되며, 성능과 화자의 수를 고려하여 충분히 큰 $C_{max}$를 사용하였다
  - 기존에 VAD(voice activity detection)과 separated speech signal을 함께 적용했을 때 성능이 향상되는 실험이 존재했다
  - 따라서 EEND-SS에서 출력되는 speaker activity $\hat{s}_c = Decoder(d_c)$에 diarization branch에서 출력되는 speech activity probabilities $p$를 사용하여 speaker가 존재하지 않은 곳의 background noise를 감소시킨다
    - $\hat{s}^\prime = \hat{s} \odot p^{\phi_{max}}$
    - $p^{\phi_{max}} := argmax_{(\phi_1, \cdots, \phi_C) \in \Phi(C)}\displaystyle\sum_{c=1}^C{r(abs(\hat{s}), p^\phi))}$
    - $r(\cdot, \cdot)$은 correlation function, $\Phi(C)$는 $(1,\cdots, C)$의 가능한 permutation, $p^\phi := [p_{\phi_{c,t}} \in (0,1) | c = 1,cdots,C]$는 permuted 사후 확률(posterior probabilities)

- training
  - $L = \lambda_1 L_{SI-SDR} + \lambda_2 L_{diar} + \lambda_3 L_{exist}$의 multi-task cost function을 사용하여 학습시킨다
  - $\lambda_1, \lambda_2, \lambda_3 \in R_{+}$는 weighting parameter로 모두 empirically하게 결정된다

- inference
  - 다양한 수의 speaker를 다루기 위해 2개의 procedure로 진행된다
  - 1.diarization probabilities $p_t$와 speakers의 수 $\hat{C}$를 얻는다
  - 2.$\hat{C}$의 값에 따른 $1\*1 conv$ layer가 결정되고, separated speech signals $\hat{s}\_1, \cdots, \hat{s}\_{\hat{C}}$를 얻는다
  - 3.추가적으로 $p_t$를 사용하여 $\hat{s}$를 $\hat{s}^\prime = \hat{s} \odot p_t$로 refining한다








