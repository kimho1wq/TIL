# One-Step Knowledge Distillation and Fine-Tuning in Using Large Pre-Trained Self-Supervised Learning Models for Speaker Verification

## Introduction

화자 인식(speaker recognition)은 일반적으로 화자 식별(speaker identification)과 화자 검증(speaker verification)으로 나뉘어진다

화자 식별(speaker identification)은 주어진 발성(utterance)로 부터 해당 화자를 찾아내는 기술이고
화자 검증(speaker verification)은 주어진 발성(utterance)가 시스템에 등록된 사용자들의 목소리 인지 아닌지 검증하는 기술이다

다른게 말하면 화자 검증은 해당 input utterance가 target speaker로 부터 발성되었는지(vocalization)을 검증하는 것이다

대부분의 화자 검증 시스템은 spectrogram, mel spectrogram, mel-frequency cepstral coefficient(MFCC)등의 hand-crafted acoustic features를 사용하였다

최근 Wav2Vec 2.0(W2V2), Hu-BERT, WavLM등, speech self-supervied learning(SSL)의 발전으로 hand-craft feature보다 더욱 유용한 특징을 추출할 수 있게 되었다

다만 W2V2(Large), HuBERT는 각각 317M, 964M개의 parameters를 가지고 있기 때문에 사용하기 어려운 단점이 있다

이를 위해서 knowledge distillation(KD)과 fine-tuning(FT)을 사용해서 모델의 사이즈를 줄이는 방법을 주로 사용한다

하지만 이러한 방법은 KD와 FT를 독립적으로 학습해야 하고, 적절한 transition point를 찾기가 힘들다는 단점이 있다

따라서 위 논문에서는 one-step으로 진행으로 SSL모델을 화자 검증에 적합한 모델로 compression하는 방법(One-Step Knowledge Distillation and Fine-Tuning, OS-KDFT)을 제안한다



## Proposed method

기존 KD와 FT training process를 one-step으로 통합하여 target task에 불필요한 정보의 distillation을 방지할 수 있다

![image](https://github.com/kimho1wq/TIL/assets/15611500/027a8447-46ff-4ac4-a2b6-941de4c325dc)

- Architecture
  - student network를 제안된 parameter로 구성한 후 SV를 위한 adapter path와 KD를 위한 non-adapter path의 별개의 routes(path)로 학습을 진행한다
  - 2개의 paths는 CNN과 encoder weights를 공유하고, adapter path의 adapter block만 별도의 weights를 가진다
  - path를 나눈 추가적인 이유는 서로 충돌(conflict)하는 2개의 multi-task(KD, FT)간의 negative transfer의 영향을 줄이기 위해서이다
    - Encoder block의 $LN+MHAT$: $f(x_i) = FeedForward(x_i)$
    - non-adapter path: $y_i = x_i + f(x_i)$
    - adapter path: $y^\prime_i = x_i^\prime + f(x^\prime_i) + ReLU(x^\prime_i W_{down})W_{up}$
      - $W_{down} \in  R^{1024\*64}$은 down-sampling을 의미하며 $W_{up} \in R^{64\*1024}$는 up-sampling를 의미한다 
- Weight initialization, learning rate
  - classifier의 parameter는 random하게 초기화하였고, CNN과 encoder는 teacher weight를 이용하여 초기화 하였다
  - 적은 수의 parameter를 가진 student의 encoder를 초기화 할 때, CNN에서 가까운(closeness) 순서대로 teacher encoder의 weights를 가져와서 초기화 하였다
  - CNN, encoder, adapter, classifier는 전부 다른 learning rate를 사용하였다 $(\tau는 $
    - classifier는 weights가 random하게 초기화되었기 때문에 cosine annealing 방법을 사용하여 learning rate가 점차 감소하는 방향으로 진행하였다
      - $lr_c^\tau = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})(1+cos(\frac{\tau}{\tau_{tot}}\phi))$
      - lr_c^\tau는 현재의 학습률이며, \eta_{min}, \eta_{max}는 학습 전에 설정하는 고정된 값으로 각각 학습률의 최소값, 최대값이고, \tau는 현재 epoch이며, \tau_{tot}는 Cosine annealing을 실행하는 주기이다
    - CNN과 encoder는 pre-trained된 weights를 사용하므로 learning rate를 10 epochs 동안은 점진적으로 증가시키고, 그 후 감소시키는 방향으로 진행하였다
      - $lr_s^\tau = lr_c^\tau \* \frac{\tau}{10}, (\tau<=10)$, $lr_s^\tau = lr_s^{\tau-1} \* \beta, (\tau>10)$ 여기서 $\beta=0.93$을 사용하였다
    - adapter는 random하게 초기화 되었기 때문에 consine annealing 방법을 사용하였고, classifier에 맞춰 학습률을 empirical하게 조정하였다
      - $lr_a^\tau = lr_c^\tau \* \theta$, 여기서 $\theta=10$을 사용하였다
      











