# LARGE-SCALE SELF-SUPERVISED SPEECH REPRESENTATION LEARNING FOR AUTOMATIC SPEAKER VERIFICATION

## Introduction

대부분의 기존 시스템들은 많은 양의 well-label된 data가 필요하기 때문에 privacy issue가 있는 real환경에서 데이터를 모아 학습시키기 힘들다

그리고 화자 검증(speaker verification) field에서 1개의 utterance에 1명의 speaker가 있다는 가정은 unlabeled data로 un-supervised learning을 진행시키는데 어려움을 주었다

최근 Wav2Vec 2.0과 HuBERT는 pre-trained model을 이용해서 speech에서 효과적인 phonetic information을 추출하여 target model을 fine-tuning하는 기법을 제안하였다

speech representation을 위한 SSL(self-supervised learning)의 과정은 3가지의 categories로 나뉠 수 있다

1. reconstruction learning: masked된 input이나 과거 시간의 정보를 이용하여 original input을 재구성한다
2. contrastive learning: 잠재(latent) embedding 공간에서 대조 작업을 해결하여 high-level representation을 학습한다
3. multi-task learning: multi objectives와 multi inputs을 이용한 학습


## Proposed method
![image](https://github.com/kimho1wq/TIL/assets/15611500/eaf3f982-6a85-486a-ad2f-dcfae4e810cc)

- Pre-train for representation learning
  - W2V2(Wav2Vec 2.0)은 true speech segment와 negatives를 구별하기 위해 contrastive loss를 사용한다
  - HuBERT는 weak supervised label을 사용하여 masked된 frames을 예측하는 방법을 사용한다 
  - UniSpeech-SAT은 HuBERT와 비슷하게 learned representation을 이용하여 speaker-related한 정보를 추출하기 위한 방법으로 utterance-wise contrastive loss를 사용한다
  - 이렇게 다른 방법으로 이용하여 pre-trained model을 학습하지만 모델의 structures는 전부 convolutional feature extractor와 transformer 구조를 사용한다
    - input waveform $X = (x_1, \cdots, x_n)^n_{t=1}$은 CNN Feature Encoder에 입력으로 들어가고, $N$은 sampling point의 개수를 의미한다
    - CNN feature encoder의 output인 feature vector의 sequence는 $H_0 = (h_{1,0}, \cdots, h_{T,0})^T_{t=1}$
    - 그 후 $H_0$는 Transformer layer에 들어가고, $l$번째 layer의 각 frame 별 hidden state는 $H_l = (h_{1,l}, \cdots, h_{T,l})^T_{t=1}, (l \in (1,\cdots, L))$이다
- Leverage Representations from pre-trained Model
  - Downstream speaker verification model
    - Squeeze-Excitation Res2Blocks이나 multi-layer aggregation을 사용하고 있는 ECAPA-TDNN을 downstream model로 활용하였다
    - ECAPA-TDNN은 Fbank features를 input으로 사용하고, input features는 frame encoder를 거쳐 각 frame에 대한 speaker information를 추출한다
    - 여기서 기존의 Fbank features를 대신하여 pre-trained model의 representations을 사용하기 위해 model의 last-layer outputs을 ECAPA-TDNN의 입력으로 사용한다 
    - 그 후 statistic pooling layer를 거쳐 input sequence를 fix-demensional representation으로 변환하고 FC layer를 이용하여 speaker embedding을 추출한다
  - Explore speaker information in pre-trained model
    - 기존의 pre-trained task는 speaker recognition objective를 직접적으로 사용하지 않고, model의 output layer와 가까워 질 수록 training loss와 밀접하게 관련된 정보가 추출될 가능성이 높다
    - 따라서 SV task에 적합한 pre-trained model의 각 layer의 영향력을 학습하기 위해 learnable weight $w_l$을 사용하였다
    - frame representation $o_t$를 만들기 위해서 각 layer의 hidden states $h_{l,t}$와 learnable weight $w_l$를 가중 평균(weighted average)하였다
      - $o_t = \displaystyle\sum_{l=0}^L w_l \odot h_{l,t}$
      - $e = ECAPA-TDNN(o_1, \cdots, o_T)$
    - 총 2개의 stage로 나눠서 학습을 진행하였고, 첫 번째 stage는 pre-trained model을 fix하여 ECAPA-TDNN과 learnable weight $w$만 학습하였다
    - 그런 다음 pre-trained model을 포함한 모든 parameters를 fine-tune 하는 방식으로 진행하였다
    - 또한 speaker identification 학습으로 additive angular margin(AAM) loss를 이용하여 모델을 학습시켰다









