# END-TO-END NEURAL SPEAKER DIARIZATION WITH SELF-ATTENTION

### Introduction
전통적인 speaker diarization은 speech actitiry detection(SAD), speaker embedding extractor, clustering을 분리하여 최적화 한다

speaker embedding은 i-vecoters, d-vectors, x-vectors 등이 있고, 
clustering은 Gaussian mixture models, agglomerative hierarchical clustering, mean shift clustering, links, spectral clustering 등이 있다

기존 clustering-based 접근 방법은 두 가지 문제가 존재했다
- 1.clustering procedure은 unsupervised learing이기 때문에, diarization error를 직접적으로 최적화 하지 못한다
- 2.clustering algorithm은 1개의 segement당 한명의 화자를 가정하기 때문에, 화자가 중첩(overlap)되어 있을 경우 처리하지 못한다
- 3.speaker embedding extractor는 single-spaker non-overlapping segment를 가정하는데, 실제 오디오 레코딩에서는 중첩된 화자를 인식 할 수 있어야 한다

이러한 문제를 해결하기 위하여 self-attentitive end-to-end neural diarization(SA-EEND)를 제안했다

SA-EEND와 기존 방법의 가장 큰 차이는 clustering 방법에 의존하지 않고, training/inference시 각 프레임당 중첩되어 있는 모든 화자의 음성 여부를 출력한다


UIS-RNN(unbounded interleaved-state recurrent neural networks)은 speaker embedding을 clustering하지 않고 최적화를 한 기법이지만
SAD와 speaker embedding은 각각 분리해서 학습시켜야 하고, 중첩된 음성은 처리하지 못하는 문제가 있다

self-attention machanism은 text processing에서 sentence embedding을 추출하기 위해 처음 제안되었고, 지금은 machine translation, video clssification, image segmentation 등 다양한 분야에서 사용되고 있다

단, network architecture에 따라 maximum speaker의 수가 정해져 있는 한계가 존재한다

### Proposed method

#### End-to-end neural diarization review
EEND(end-to-end neural diarization)은 기존 diarization task를 multi-label classification으로 공식화 하였다

하나의 음성 신호로 부터 $T$길이의 observation sequence $X = (x_t \in R^F | t = 1, \cdots T)$가 주어졌을 때,
speaker diaization problem은 $Y = (y_t = [y_{t,c} \in 0,1 | c = 1, \cdots, C]|t=1, \cdots T)$에 일치하는 $X$를 찾는 것

$x_t$는 $t$시간에 관찰된 $F$차원의 feature vector이고, $C$는 $t$시간에 존재하는 speaker의 수 (t시간에 overlap된 화자 수)

여기서 frame-wise posterior인 $P(Y|X) \approx \displaystyle\prod_t\displaystyle\prod_c{P(y_{t,c}|X)}$를 기존 방법으로는 BLSTM based neural network로 최적화 시켜서 계산하였다

하지만 SA-EEND에서는 BLSTM 대신 attention-based encoding block을 사용한다


#### Self-attention-based neural network

![image](https://github.com/kimho1wq/TIL/assets/15611500/b02ec0c3-6bad-497c-a12a-9b322decb5d8)
 
- Variable
  - T : The total number of frame (500 if 50 sec)
  - F : Feature dimension (345)
  - D : Encoder hidden dimension (256)
  - S : Fixed-number of Speaker (2)
  - P : Number of Encoder Block (4)
- Stacked sub-sampling: raw input -> T x F
  - STFT + log-mel: raw input -> 10T x 23
    - window size: 25ms
    - hop size: 10ms
    - log-mel: 23 log-mel
  - Stacked-frame: 10T x 23 -> 10T x F(345) (F: 23 log-mel x 15 = 345)
    - context length: 15 samples (previous 7 sample + current 1 sample + next 7 sample)
  - Sub-sampling: 10T x F -> T x F
    - Step size: 10, Sampled 100ms per frame (hop size 10ms x 10 = 100ms)
- SA-EEND: T x F(345) -> T x D(256)
  - FC layer: T x F -> T x D (Transformer 입력을 위한 embedding 과정)
  - Encoder Blocks: T x D -> T x D
    - P개의 Transformer(self-attention) 로 구성 됨
    - self-attention mechanism 기반 모든 Frame 간 관계 해석을 통한 embedding encoding 과정
  - Layer Norm: T x D -> T x D
  - Linear: T x D(256) -> T x S(2)
    - Encoding 된 Speaker Embedding 의 각 Frame 에 대해서 Linear 을 이용하여 hidden dim(256)을 화자 수(S)로 project 한다.
    - 이때, Linear 의 weight인 D x S 을 각 화자 S 명의 1 x D 길이의 attract vector로도 해석 할 수 있다. (이해 하지 않아도 문제 없음)

- Stacked sub-sampling (Input Frame)
  - 10ms 마다 생성된 log-mel feature를 Transformer의 그대로 넣어서 사용하기에는 메모리뿐만 아니라 각 Frame이 화자 특징을 대표하기에는 너무 짧은 시간이다
  - 그래서 본 논문은 이전 BLSTM-EEND의 입력형태인 Stacked sub-sampling (Input Frame)을 이용한다
  - Hyper-Parameter
    - ![image](https://github.com/kimho1wq/TIL/assets/15611500/eb1a1301-1f2c-4189-9d58-f74bcffd2045)
    - Context Size(CS): 현재 프레임의 Context Size만큼 이전과 이후 feature들을 Stacking하여 사용하는 방법이다. 본 논문은 7를 사용하였다.
    - Sub-Sampling(SS): 일정 Frame 마다 Sub-Sampling 하여 사용하는 방법이다. 본 논문은 10를 사용하고, 이는 한 Frame이 100ms를 대표하는 것으로 볼 수 있다

- Encoder Block: Self-Attention (Transformer)
  - Variable
    - T : The total number of frame (500 if 50 sec)
    - D : Encoder hidden dimension (256)
    - d_ff : FFN internal units (1024)
  - Multi-head self-attention (MHSA)
    - Query, Key, Value 에 동일한 입력을 넣어준다.
    - Number of Header: 4
  - Position-wise FF (FFN)
    - 1st Linear: T x D(256) -> T x d_ff(1024)
    - 2nd Linear: T x d_ff(1024) -> T x D(256)

input features : $e_t^{(0)} = W_0w_t + b_o \in R^D$, $e_t^{(p)} = Encoder_t^{(p)}(e_1^{(p-1)}, \cdots, e_T^{(p-1)} (1 <= p <= P)$, $W_0 \in R^{D\*F}$, $b_0 \in R^D$

$Encoder_t^{(p)}(\dot)$은 $p$번째 encoder block으로 총 $P$개의 인코더 블럭을 사용하며, $D$차원의 input vector sequence를 받아 $t$시간의 $e_t^{(p)}$ vector 를 출력한다

encoder block은 2개의 sub-layer로 multi-head self-attention layer, position-wise feed-forward layer를 가진다

- 1.layer normalization을 통해 sequence of vectors $(e_t^{(p-1)} | t = 1, \cdots , T)$가 $R^{TxD}$ 행렬로 변환된다
  - $\bar{E}^{(p-1)} = LayerNorm([e_1^{(p-1)} \cdots e_T^{(p-1)}]^T) \in R^{T\*D}$
- 2.그리고 각 head에서 pairwise similarity matrix $A_h^{(p)}$는 query vectors과 key vectors의 dot products로 계산된다
  - $A_h^{(p)} = \bar{E}^{(p-1)}Q_h^{(p)}(\bar{E}^{(p-1)}K_h^{(p)})^T \ in R^{TxT}$, $(1<=h<=H)$
  - $Q_h^{(p)}, K_h^{(p)} \in R^{D\*d}$는 각각 $h$개의 head를 가지고 있는 query와 key projection matrix이고 $H$는 head의 개수, $d = D/H$는 각 해드의 차원이다
- 3.pairwise similarity matrix $A_h^{(p)}$는 attention weight matrix에 구성되기 위해 scaled된다
  - $\hat{A}_h^{(p)} = Softmax(\frac{A_h^{(p)}}{\sqrt{d}}) \in R^{T\*T}$
- 4.context vectors $C_h^{(p)}$는 value vectors의 weighted sum을 이용하여 계산된다
  - $C_h^{(p)} = \hat{A}_h^{(p)}(\bar{E}^{(p-1)}V_h^{(p)}) \in R^{T\*d}$
  - $V_h^{(p)}$는 value projection matrix이다
- 5.context vectors는 concatenate한 후 output projection matrix를 이용하여 계산된다
  - $E^{p, SA} = [C_1^{(p)} \cdots C_H^{(p)}]O^{(p)} \in R^{T\*D}$
- 6.redisual connection과 layer normalization을 적용한다
  - $\bar{E}^{(p, SA)} = LayerNorm(\bar{E}^{(p-1)} E^{(p, SA)}) \in R^{T\*D}$

position-wise feed-forward layer는 $\bar{E}^{(p, SA)}$를 $E^{p, FF} = ReLU(\bar{E}^{(p, SA)}W_1^{(p)} + b_1^{(p)}1)W_2^{(p)} + b_2^{(p)}1 \in R^{T\*D}$의 식으로 변형시킨다

- $W_1^{(p)} \in R^{D\*d_{ff}}, b_1^{(p)} \in R^{d_{ff}}$, $W_2^{(p)} \in R^{d_{ff}\*D}, b_2^{(p)} \in R^{D}$는 각각 1,2번째 linear projection matrix와 bias이다
- $1 \in R^{1\*T}$은 all-one row vector이고, $d_{ff}$는 internal units의 개수이다

각각의 time frame에 대해 encoder block의 output인 $e_t^{(p)}$
- $[e_1^{(p)} \cdots e_T^{(p)}] = (\bar{E}^{(p, SA)} + E^{(p, FF)})^T$

frame-wise posterior $z_t$의 계산
- $\bar{E}^{(P)} = LayerNorm([e_1^{(P)} \cdots e_T^{(P)}]^T) \in R^{T\*D}$
- $[z_1 \cdots z_T] = \sigma(\bar{E}^{(P)} W_3 + b_3 1)^T$, $(W_3 \in R^{D\*C}, b_3 \in R^C)$



#### Permutation-free training
같은 화자지만 화자의 순서가 바뀌는 경우(다른 permutation이 될 경우) label이 모호(ambiguity)해지기 때문에 permutation-free loss fuction(permutation invariant training, PIT)을 제안했다

permutation-free loss는 output $z_t$과 speaker label $l_t$간 BCE(binary cross entropy)를 최소화 하는 방향이다
- $J^{PF} = \frac{1}{TC} min_{\phi \in perm(C)}\displaystyle\sum_t{BCE(l_t^\phi, z_t)}$
- $perm(C)$는 $1, \cdots C$ 중 가능한 모든 permutations 이고, $l_t^\phi$는 $\phi$번 째 permutation label이다
만약 (label 1, label 2)와 (label 2, label 1) 으로 총 2 가지가 존재한다면, 더 작은 값을 가지는 Permutation 2 (label 2, label 1)의 loss를 선택한다

### Experimental setup
2개의 학습셋, 5개의 테스트 셋을 사용하였으며, overlap 비율은 $\frac{같은 시간에 2명 이상의 화자가 존재할 때의 오디오 시간}{1명 이상의 화자가 존재할 때의 오디오 시간}을 계산하였다
![image](https://github.com/kimho1wq/TIL/assets/15611500/9606b6ca-f34c-4f58-a06b-2dfa88d6c76d)

#### Simulated mixtures
![image](https://github.com/kimho1wq/TIL/assets/15611500/45b86cae-66cc-45ce-afed-0fbda72f460b)

- $N_{spk}$: 섞을 화자 수 (Mixture 개수), $N_{umin}/N_{umax}$: 각 화자 Mixture 내 최대/최소 발화(utterance) 개수, $\beta$: 각 화자 Mixture 내 화자 간 발화 평균 간격
- 전체 화자들(Training set 기준 5,743 명) 중 $N_{spk}$ (2명) 수 만큼 Random sampling
- 각 화자에 대한 Mixture 생성
  - $i$: 10,000개 RIR 중 1개 랜덤 선택
  - $N_u$: Mixture 내 사용할 화자 발화(utterance) 개수를 $N_{umin}$과 $N_{umax}$ 사이 값 랜덤 선택
  - 한 화자 Mixture 생성
    - 이전 까지 생성된 mixture 사이에 간격(interval)을 $\delta$(delta)을 sampling 한다
    - 여기서 $\delta$인 간격은 $\beta$ 값에 비례한다
    - 이전 까지 생성된 mixture 사이에 $\delta$ 만큼에 간격(non-speech)을 준다
    - 이후 RIR를 적용한 utterance 를 mixture 뒤에 이어 붙인다
  - 생성된 화자별 Mixture를 더하여 하나의 Mixture y 생성
  - MUSAN noise 와 SNR을 랜덤 선택
  - noise recording 길이를 mixture y 길이에 맞추기 위한 mixing scale p를 구한다
  - noise을 mixing scale 후 mixture y 에 더해준다

silence interval은 $\beta$에 의해 관리되고, 각 화자당 수십개의 발성이 mixture 존재한다

Switchboard(SWBD)-2, Switchboard Cellular, NIST Speaker Recognition Evaluation(SRE) dataset 등의 telephone speech (8kHz)로 구성되어 있다

training set은 5743명의 speaker test set은 638명의 speaker이며, training set은 [Kaldi CALLHOME diarization v2 recipe](https://github.com/kaldi-asr/kaldi/tree/master/)과 같기 때문에 x-vector와의 비교가 가능하다

background noise로 MUSAN corpus중 37개의 recordings 사용했으며, 10000개의 RIRs(room impulse responses) data는 the Simulated Room Impulse Response Database에서 가져왔다

각 화자당 10~20개의 발성으로 two-speaker mixtures를 만들고($N_{spk}=2, N_{umin}=10, N_{umax}=20$), overlap 비율은 19.5%($\beta = 5$)~34.4%($\beta = 2$) 이다



















