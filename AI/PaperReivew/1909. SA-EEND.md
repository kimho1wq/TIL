# END-TO-END NEURAL SPEAKER DIARIZATION WITH SELF-ATTENTION

### introduction
전통적인 speaker diarization은 speech actitiry detection(SAD), speaker embedding extractor, clustering을 분리하여 최적화 한다

speaker embedding은 i-vecoters, d-vectors, x-vectors 등이 있고, 
clustering은 Gaussian mixture models, agglomerative hierarchical clustering, mean shift clustering, links, spectral clustering 등이 있다

기존 clustering-based 접근 방법은 두 가지 문제가 존재했다
- 1.clustering procedure은 unsupervised learing이기 때문에, diarization error를 직접적으로 최적화 하지 못한다
- 2.clustering algorithm은 1개의 segement당 한명의 화자를 가정하기 때문에, 화자가 중첩(overlap)되어 있을 경우 처리하지 못한다
- 3.speaker embedding extractor는 single-spaker non-overlapping segment를 가정하는데, 실제 오디오 레코딩에서는 중첩된 화자를 인식 할 수 있어야 한다

이러한 문제를 해결하기 위하여 self-attentitive end-to-end neural diarization(SA-EEND)를 제안했다

SA-EEND와 기존 방법의 가장 큰 차이는 clustering 방법에 의존하지 않고, training/inference시 각 프레임당 중첩되어 있는 모든 화자의 음성 여부를 출력한다


UIS-RNN(unbounded interleaved-state recurrent neural networks)은 speaker embedding을 clustering하지 않고 최적화를 한 기법이지만
SAD와 speaker embedding은 각각 분리해서 학습시켜야 하고, 중첩된 음성은 처리하지 못하는 문제가 있다

self-attention machanism은 text processing에서 sentence embedding을 추출하기 위해 처음 제안되었고, 지금은 machine translation, video clssification, image segmentation 등 다양한 분야에서 사용되고 있다

### review
EEND(end-to-end neural diarization)은 기존 diarization task를 multi-label classification으로 공식화 하였다

하나의 음성 신호로 부터 $T$길이의 observation sequence $X = (x_t \in R^F | t = 1, \cdots T)$가 주어졌을 때,
speaker diaization problem은 $Y = (y_t = [y_{t,c} \in 0,1 | c = 1, \cdots, C]|t=1, \cdots T)$에 일치하는 $X$를 찾는 것

$x_t$는 $t$시간에 관찰된 $F$차원의 feature vector이고, $C$는 $t$시간에 존재하는 speaker의 수 (t시간에 overlap된 화자 수)

여기서 frame-wise posterior인 $P(Y|X) \approx \displaystyle\prod_t\displaystyle\prod_c{P(y_{t,c}|X)}$를 기존 방법으로는 BLSTM based neural network로 최적화 시켜서 계산하였다

하지만 SA-EEND에서는 BLSTM 대신 attention-based encoding block을 사용한다

### proposed method
![image](https://github.com/kimho1wq/TIL/assets/15611500/b02ec0c3-6bad-497c-a12a-9b322decb5d8)
 
input features : $e_t^{(0)} = W_0w_t + b_o \in R^D$, $e_t^{(p)} = Encoder_t^{(p)}(e_1^{(p-1)}, \cdots, e_T^{(p-1)} (1 <= p <= P)$, $W_0 \in R^{D\*F}$, $b_0 \in R^D$

$Encoder_t^{(p)}(\dot)$은 $p$번째 encoder block으로 총 $P$개의 인코더 블럭을 사용하며, $D$차원의 input vector sequence를 받아 $t$시간의 $e_t^{(p)}$ vector 를 출력한다

encoder block은 2개의 sub-layer로 multi-head self-attention layer, position-wise feed-forward layer를 가진다

- 1.layer normalization을 통해 sequence of vectors $(e_t^{(p-1)} | t = 1, \cdots , T)$가 $R^{TxD}$ 행렬로 변환된다
  - $\bar{E}^{(p-1)} = LayerNorm([e_1^{(p-1)} \cdots e_T^{(p-1)}]^T) \in R^{T\*D}$
- 2.그리고 각 head에서 pairwise similarity matrix $A_h^{(p)}$는 query vectors과 key vectors의 dot products로 계산된다
  - $A_h^{(p)} = \bar{E}^{(p-1)}Q_h^{(p)}(\bar{E}^{(p-1)}K_h^{(p)})^T \ in R^{TxT}$, $(1<=h<=H)$
  - $Q_h^{(p)}, K_h^{(p)} \in R^{D\*d}$는 각각 $h$개의 head를 가지고 있는 query와 key projection matrix이고 $H$는 head의 개수, $d = D/H$는 각 해드의 차원이다
- 3.pairwise similarity matrix $A_h^{(p)}$는 attention weight matrix에 구성되기 위해 scaled된다
  - $\hat{A}_h^{(p)} = Softmax(\frac{A_h^{(p)}}{\sqrt{d}}) \in R^{T\*T}$
- 4.context vectors $C_h^{(p)}$는 value vectors의 weighted sum을 이용하여 계산된다
  - $C_h^{(p)} = \hat{A}_h^{(p)}(\bar{E}^{(p-1)}V_h^{(p)}) \in R^{T\*d}$
  - $V_h^{(p)}$는 value projection matrix이다
- 5.context vectors는 concatenate한 후 output projection matrix를 이용하여 계산된다
  - $E^{p, SA} = [C_1^{(p)} \cdots C_H^{(p)}]O^{(p)} \in R^{T\*D}$
- 6.redisual connection과 layer normalization을 적용한다
  - $\bar{E}^{(p, SA)} = LayerNorm(\bar{E}^{(p-1)} E^{(p, SA)}) \in R^{T\*D}$

position-wise feed-forward layer는 $\bar{E}^{(p, SA)}$를 $E^{p, FF} = ReLU(\bar{E}^{(p, SA)}W_1^{(p)} + b_1^{(p)}1)W_2^{(p)} + b_2^{(p)}1 \in R^{T\*D}$의 식으로 변형시킨다

$W_1^{(p)} \in R^{D\*d_{ff}}, b_1^{(p)} \in R^{d_{ff}}$, $W_2^{(p)} \in R^{d_{ff}\*D}, b_2^{(p)} \in R^{D}$는 각각 1,2번째 linear projection matrix와 bias이다

$1 \in R^{1\*T}$은 all-one row vector이고, $d_{ff}$는 internal units의 개수이다

마지막으로 각각의 time frame에 대해 encoder block의 output인 $e_t^{(p)}$은 $[e_1^{(p)} \cdots e_T^{(p)}] = (\bar{E}^{(p, SA)} + E^{(p, FF)})^T$














