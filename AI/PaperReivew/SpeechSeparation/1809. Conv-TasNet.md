# Conv-TasNet: Surpassing Ideal Time-Frequency Magnitude Masking for Speech Separation

## Introduction

대부분의 이전 speech separation 접근법은 time-frequency(e.g. spectrogram) representation 에서 표현되며 independent component analysis(ICA), time-domain non-negative matrix factorization(NMF) 등이 있다

이러한 접근 방법은 non-linear regression 기술로 mixture spectrogram으로 부터 각각의 speech에 대한 clean한 spectrogram을 추출하는 접근 방법이다

a weighting function (mask)을 사용하여 spectrogram의 bin에 multiply하고, inverse short-time Fourier transform(iSTFT)를 활용하여 individual source들을 복원할 수 있다

하지만 이러한 방법은 몇가지 단점이 있다
1. STFT는 일반적인 signal transformation 기술로 speech separation에 optimal한 방법이 아니다
2. clean한 spectrogram의 위상(phase)를 재구성하는 일은 어려운 문제로 정확도에 상한선(upper bound)가 생긴다
3. time-frequency representation의 복원은 high-resolution frequency가 필요하고, 이것은 STFT의 long temporal window size의 계산을 필요로 하므로 지연 시간(latency)이 많이 들게된다

최근 speech separation은 STFT의 feature extraction을 deep learning을 이용한 data-driven representation으로 대채하는 방법이 연구되고 있다

딥러닝을 이용한 방법은 feature extraction과 separation의 동작을 암시적으로(implicitly) 하나의 네트워크에서 동시에 최적화 하는 방식으로 진행된다

이러한 방법으로는 time-domain audio separation network(TasNet)가 있고, 이 네트워크는 mixture된 waveform을 convolutional encoder-decoder architecture에 넣어 output으로 각각의 wavefomr을 추출하는 형태로 학습된다

TasNet은 이전 time-frequency speech separation method의 성능을 뛰어 넘었지만 LSTM(long short-term memory)을 사용하여 발생하는 단점도 존재한다
1. 인코더에서 작은 kernel size(waveform segments의 길이)를 선택할수록 encoder의 output 길이가 증가하여 training하기 힘들다
2. 많은 수의 LSTM parameter가 필요하므로 computational cost가 증가하여 low-resource, low-power platform에서 사용하기 힘들다
3. LSTM의 long temporal dependency로 인해 일관되지 않은 separation이 발생할 수 있다

따라서 이 논문에서는 temporal convolutional network(TCN)을 이용한 Conv-TasNet을 제안하며, stacked dilated 1-D convolutional block들을 이용하여 LSTM network를 대채하였다

또한 computational cost를 감소시키기 위해 depthwise separable convolution을 사용하였다


## Proposed Conv-TasNet(Convolutional Time-domain Audio Separation Network)
![image](https://github.com/kimho1wq/TIL/assets/15611500/e28c2266-ea69-4cae-83ef-9d90abd5e72f)

Conv-TasNet은 3개의 precessing stage로 나뉘며 각각 encoder, separation, decoder이다

encoder module은 mixture waveform을 intermediate feature space로 전달하며, 이 representation은 multiplicative function(mask)를 계산하는데 사용되고, 이 masked encoder features를 이용하여 decoder에서 source waveform을 reconstruction한다


### Time-domain speech separation

single-chaneel speech separation은 mixture waveform $x(t) \in R^{1\*T}$가 주어졌을 때 $C$ sources인 $s_1(t), \cdots, s_c(t) \in R^{1\*T}$를 계산하는 문제이다 $(x(t) = \displaystyle\sum_{i=1}^C{s_t(t)})$ 

### Convolutional encoder-decoder

$x_k \in R^{1\*L}$는 input으로 mixture sound를 length $L$인 segments로 나눈것이며, $k = (1, \cdots, \hat{T})$는 segment index이고, $\hat{T}$는 segments의 총 갯수이다

$x_k$는 1-D convolution operation( $w = \eta(xU)$ )을 통해 $w \in R^{1\*N}$인 $N$차원 representation으로 변환된다

$U \in R^{N\*L}$은 각각 $L$개의 길이를 가지는 $N$개의 vector(encoder basis function)이고, $\eta(\cdot)$은 optional nonlinear function이다

decoder는 1-D convolution operation($\hat{x} = wV$)을 통해 $x$를 reconstruction할 수 있고, $V \in R^{N\*L}$은 decoder basis function이다


### Estimating the separation masks

mask vector $m_i \in R^{1\*N}$는 $m_i \in [0,1]$이며, $i = (1, \cdots, C)$이고, $C$는 mixture된 speaker의 수이다

각각 source의 representation $d_i \in R^{1\*N}$은 $d_i = w \odot m_i$로 계산되며, $\odot$은 element-wise multiplication이고, $w$는 mixture representation이다

각각 source의 waveform $\hat{s}_i, (i=1,\cdots,C)$는 $\hat{s}_i = d_i V$로 계산된다 

$\displaystyle\sum_{i=1}^C{m_i} = 1$은 unit summation constraint으로 사용된다


### Convolutional separation module

1-D dilated convolutional block으로 구성되어 있는 fully-convolutional separation module을 제안했다

기존 temporal convolutional network(TCN)은 다양한 sequence modeling을 통해 RNN을 대채했었고, 이 논문의 네트워크 또한 dilation factor를 지수적(exponentially)으로 증가시키는 구조를 통해 충분한 양의 temporal context를 활용할 수 있도록 하였다

Conv-TasNet은 $1,2,4,\cdots,2^{M-1}$의 $M$ convolutional block을 $R$번 반복하고, 각 block의 input은 zero padding을 추가하여 input의 길이와 output의 길이가 같도록 만든다

TCN의 output은 kenel size가 1인 convolutional block(pointwise convolution)을 통과하여 $C$명의 target source를 위한 $C$개의 mask vectors가 만들어 진다

nonlinear activation function은 parametric rectified linear unit (PReLU)을 사용했고, $PReLU(x) = if x >= 0 then x, otherwise \alpha x$이며 $\alpha$는 trainable scalar이다

separation module의 시작은 $1\*1-conv$ block이고, 이 block은 그 다음의 input, residual path의 channel의 수를 결정한다

- $1\*1-conv$ block이 $B$ channel, 1-D convolutional block이 $H$ channel과 kernel size $P$ 일 경우 1-D conv block안의 kernel size
  - $1\*1-conv$ block의 kernel size: $O \in R^{B\*H\*1}$
  - $D-conv$ block의 kernel size: $K \in K^{H\*P}$
  - residual path의 kernel size: $L_{R_s} \in R^{H\*B\*1}$
  - skip-connection path의 out channel의 수는 $B$와 다르기 때문에 skip-connection의 kernel size: $L_{S_c} \in R^{H\*S_c\*1}$







