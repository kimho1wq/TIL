# Introduction: Information Theory(IT)

### 정보이론
- 신호(Signal)에 존재하는 정보(Information)의 양을 착정하는 응용수학의 한 분야
- 특히 머신러닝에서는 해당 확률분포(Probability Distribution)의 특성을 알아내거나, 확률분포 간 유사성을 정량화(Quantification)하는데 쓰인다.
- 머신러닝과 정보이론은 같은 도구(엔트로피나 베이지안 확률)을 사용한다. ML and IT both use the same tools.
- 정보이론은 실용적이거나 실제적인 엔트로피의 해석을 제공한다. IT provides "practical" or "realistic" interpretation of "entropy".
  - entropy <-> uncertainty <-> information
- 정보이론은 정보의 수학적인 정의이다. IT provides mathmatical definition of information.


### 섀넌 엔트로피(Shannon Entropy)
- 정보이론(섀넌 엔트로피)의 핵심 조건
  1. 잘 일어나지 않는 사건(unlikely event, 발생 확률이 0에 가까울 수록)는 자주 발생하는 사건보다 정보량이 많다.
  2. 발생이 보장된 사건(발생 확률이 1에 가까울 수록)은 그 내용에 상관없이 정보가 전혀 없다.
  3. 독립사건(independent event)은 추가적인 정보량(additive information)을 가진다.
    - Ex) 동전을 던져 앞면이 두번 나오는 사건의 정보량 = 2 x 동전을 던져 앞면이 한번 나오는 사건의 정보량
- 확률변수 X의 값이 x인 사건의 정보량
  - $I(x) = -\log P(x)$
    - Ex) 동전을 던져 앞면이 나오는 사건의 정보량 $I = -\log_2{\frac{1}{2}} = 1
    - Ex) 주사위를 던져 눈이 1이 나오는 사건의 정보량 $I = -\log_2{\frac{1}{6}} = 2.59$
- Shannon Entropy
  - 확률 분포에서의 불확실성 또는 무작위성의 정도를 측정하는 방법 중 하나 
  - 어떤 확률분포 $P$에 대한 모든 사건 정보량의 기대값 $H(P)$을 의미
    - $H(P) = H(x) = E_{X~P}[I(x)] = E_{X~P}[-\log P(x)]$: X 확률의 음의 로그로 정의됨
    - 단위: log의 밑이 2인 경우(bit or shannon), 자연상수(exp)인 경우(nat)
      - Ex) 동전을 던져 앞면이 나오는 사건의 정보량
        - $H(x) = -\sum_{x}{P(x)\log P(x)} = -(0.5 \times \log_2{0.5} | 0.5 \times \log_{2}{0.5} = 1비트(bit)$ 
    - 각각의 확률값으로 가중평균한 것이 H(X)가 되므로, 각 확률값이 정보의 "기여도"를 나타내고, X의 분포가 얼마나 예측하기 어려운지를 측정하는 지표이다.
  - Binary Entropy
    - 이항 분포에서 나오는 확률 분포 X에 대한 섀넌 엔트로피를 계산한 것
    - X가 1일 확률이 p, 0일 확률이 1-p: $H(X) = -[p\log (p) + (1-p)\log (1-p)$
    - 섀넌 엔트로피의 변화 그래프 (x축은 앞면이 나올 확률, y축은 엔트로피)
      - 확률이 0.5일 경우 가장 큰 엔트로피(불확실성이 가장 큼)
      - ![image](https://github.com/kimho1wq/TIL/assets/15611500/b94b9ae1-805d-466c-91d4-2c9f1c4efabc)
  
### 쿨백-라이블러 발산(Kullback-Leibler divergence, KLD)
- 두 확률분포($P,Q$)의 차이를 계산하는 데 사용되는 함수
  - $D_{KL}(P||Q) = E_{X~P}[\log{\frac{P(x)}{Q(x)}}] = E_{X~P}[-\log{\frac{Q(x)}{P(x)}}$
- P와 Q가 동일한 확률분포일 경우 $D_{KL}(P||Q)&의 값이 0이 되지만, 비대칭(not symmetric) 수식이기 때문에 P,Q의 위치가 바뀌면 값이 달라진다 -> 즉, 거리함수로 사용할 수 없다.


### 크로스 엔트로피(Cross Entropy)
- Binary Cross Entropy
  - 분류 모델의 손실 함수 중 하나로, 실제 값($y$)과 예측 값($\hat{y}$)이 0 또는 1인 이진 분류 문제에서 사용
    - $J = -[y\log (\hat{y}) + (1-y)\log (1-\hat{y})$
  - $J$의 값은 예측한 값이 실제 값과 일치할 때 최소가 된다.
- Cross Entropy
  - 실제 확률분포($y$)와 예측 확률분포($\hat{y}$)를 비교하는 데 사용되는 두 확률 분포의 차이를 측정하는 지표
  - $C$는 모든 카테고리(or 클래스)의 갯수로, 입력을 여러 카테고리 중 하나로 분류하는 모델의 학습에 주로 사용됨
    - $J = -\sum_{k=1}^C{y_k\log (\hat{y_k})}$


### Bent coin clip
Goal: we will bring "entropy" to 이항분포(binormial distribution).
- P[uphead] = f
- P[downhead] = 1-f
- Toss n times
- P(r|N,f) = nCr * f^r (1-f)^{N-r}
  - nCr = N!/(r! * (N-r)!)

- <r> = E[r] = sum_{r=0}^{N}{r * P(r|N,f)}
  - sum_{r=0}^{N}{r *(N!/(r!(N-r)!) * f^r(1-f)^(N-r)}
  - sum_{M=0}^{N-1}{ (N*(N-1)!)/(M!(N-(M-1))! * f^M * f*(1-f)^{N-(M+1)}}


### What is entropy

- Energy는 보존된다. Irreversible phenomena 발생 (유리컵이 깨지면 안붙음, 잉크가 펴져나감)
- Heat 물리 (Entropy as irreversibility)
- Statistical Machanism 물리 (Entropy as disorder)
- Information Theory (Entropy as ignorance) (information and memory)
- Entropy is unique, continuuous function satistying following 3 properties.
  1. Maximum for equal probailities (정보가 전부 유사할때 불확실성이 최대가 된다)
  2. Unaffected by extra states of zero probability
  3. Entorpy change for conditional probailities


### How can we achieve perfect communication over an inperfect noisy communication channel?















