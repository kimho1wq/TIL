# Introduction: ML, IT

### Why study information theroy and machine learning
- 머신러닝과 정보이론은 같은 도구(엔트로피나 베이지안 확률)을 사용한다. ML and IT both use the same tools.
- 정보이론은 실용적이거나 실제적인 엔트로피의 해석을 제공한다. IT provides "practical" or "realistic" interpretation of "entropy".
  - entropy <-> uncertainty <-> information
- 정보이론은 정보의 수학적인 정의이다. IT provides mathmatical definition of information.

- What is ML/AI?
  - building machines (functions) that produce text/images (for the given)
- How to handle such information? mathmatical foundation?
- How to integrate prior knowledges.
- How to make an optimal decision.

### Bent coin clip
Goal: we will bring "entropy" to 이항분포(binormial distribution).
- P[uphead] = f
- P[downhead] = 1-f
- Toss n times
- P(r|N,f) = nCr * f^r (1-f)^{N-r}
  - nCr = N!/(r! * (N-r)!)

- <r> = E[r] = sum_{r=0}^{N}{r * P(r|N,f)}
  - sum_{r=0}^{N}{r *(N!/(r!(N-r)!) * f^r(1-f)^(N-r)}
  - sum_{M=0}^{N-1}{ (N*(N-1)!)/(M!(N-(M-1))! * f^M * f*(1-f)^{N-(M+1)}}


### What is entropy

- Energy는 보존된다. Irreversible phenomena 발생 (유리컵이 깨지면 안붙음, 잉크가 펴져나감)
- Heat 물리 (Entropy as irreversibility)
- Statistical Machanism 물리 (Entropy as disorder)
- Information Theory (Entropy as ignorance) (information and memory)
- Entropy is unique, continuuous function satistying following 3 properties.
  1. Maximum for equal probailities (정보가 전부 유사할때 불확실성이 최대가 된다)
  2. Unaffected by extra states of zero probability
  3. Entorpy change for conditional probailities















