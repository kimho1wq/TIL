# 순환 신경망

### 순환 신경망(recurrent neural network, RNN)
- 순환 신경망의 구조
  - 순차 데이터를 처리하는 신경망은 다음 세 가지 기능을 갖추어야 한다
    - 시간성: 특징을 순서대로 한 번에 하나씩 입력해야 한다
    - 가변 길이: 길이가 $T$인 샘플을 처리하려면 은닉층 $T$번 나타나야 하며 $T$는 가변적이다
    - 문맥 의존성: 이전 특징 내용을 기억하고 있다가 적절한 순간에 활용해야 한다
  - RNN(recurrent neural network)은 순환 에지(recurrent edge)를 가짐으로써 시간성, 가변길이, 문맥 의존성이라는 세 가지 기능을 모두 갖춘다
    - ![image](https://github.com/kimho1wq/TIL/assets/15611500/a352c0a5-dfe8-48fc-a06e-9c36988d820a)
  - 시간 연결성을 순환식으로 표현: $h^{(t)} = f(h^{(t-1)}, x^{(t)}; \Theta)$, $\Theta$는 RNN의 가중치집합으로서 $U, V, W$에 해당된다
  - RNN의 은닉층은 직전 순간의 은닉층 $t-1$에 영향을 받으며, 이전 정보를 기억하는 역할을 수행하는 상태 변수 라고 한다
  - 입력층, 은닉층, 출력층의 노드 개수를 각각 $d, p, q$라 하면, $U$는 $p\*d$, $W$는 $p\*p$, $V$는 $q\*p$, $b$는 $p\*1$, $c$는 $q*1$ 이다
  - 가중치 공유의 장점
    - 1.학습 과정이 추정할 매개변수의 수를 획기적으로 줄여 최적화 문제를 합리적인 크기로 유지한다
    - 2.매개변수의 개수가 특징 벡터의 길이 $T$와 무관하게 일정하다
    - 3.특징이 나타나는 순간이 뒤바뀌더라도 같거나 유사한 출력을 만들 수 있다
- 순환 신경망의 동작
  - $a^{(t)} = Wh^{(t-1)} + Ux^{(t)} + b$
  - $h^{(t)} = \tau(a^{(t)})$
  - $o^{(t)} = Vh^{(t)} + c$
  - $\hat{y}^{(t)} = softmax(o^{(t)})$
- 시간 역전파 알고리즘(back-propagation through time, BPTT)
  - RNN의 목적함수 $J(\Theta) = \displaystyle\sum_{t=1}^T{J^{(t)}(\Theta)}$로, $J^{(T)}(\Theta)$는 순간 $t$에서 오류를 추정하는 함수이다
  - SGD의 최소점을 찾아가기 위해선 $\frac{\partial J}{\partial \Theta} = \frac{\partial J}{\partial U}, \frac{\partial J}{\partial W}, \frac{\partial J}{\partial V}, \frac{\partial J}{\partial b}, \frac{\partial J}{\partial c} $가 필요하다
    - ![image](https://github.com/kimho1wq/TIL/assets/15611500/92a51025-0d8a-4302-a3f1-57f36ad98c64)
  - $v_{ji}$의 한 요소에 대한 $\frac{\partial J}{\partial v_{ji}}$에 대한 식
    - $V$는 순간마다 출력에만 영향을 주고, 은닉층의 값에는 전혀 영향을 주지 않아서 순환식으로 표현하지 않아도 된다
    - $\frac{\partial J^{(t)}}{\partial v_{12}} = \frac{\partial J^{(t)}}{\partial y^{\prime(t)}} \cdot \frac{\partial y^{\prime(t)}}{\partial o_1^{(t)}} \cdot \frac{\partial o_1^{(t)}}{\partial v_{12}}$
    - $o_1^{(t)} = v_1h^{(t)} = v_{11}h_1^{(t)} + v_{12}h_2^{(t)} + v_{13}h_3^{(t)}$이므로, $\frac{\partial o_1^{(t)}}{\partial v_{12}} = h_2^{(t)}$이다
    - $J^{(t)}$의 경우 로그우도를 사용할 때 목푯값이 $y^{(t)} = (1,0)^T$인 경우 $J^{(t)} = -\log{y_1^{\prime (t)}}$이고, $y^{(t)} = (0,1)^T$인 경우 $J^{(t)} = -\log{y_2^{\prime (t)}}$, 여기서 $y^{\prime(t)|$은 출력값 
      - $y^{(t)} = (1,0)^T$인 경우, $\frac{\partial J^{(t)}}{\partial o_1^{(t)}} = -1 + y_1^{\prime(t)}$ (softmax를 사용 $y_1^{\prime(t)} = \frac{exp(o_1^{(t)})}{exp(o_1^{(t)})+exp(o_2^{(t)})}$)
      - $y^{(t)} = (0,1)^T$인 경우, $\frac{\partial J^{(t)}}{\partial o_1^{(t)}} = y_1^{\prime(t)}$
    - $v_{12}$를 $v_{ji}$로 일반화 하고, $y^{(t)}$의 요소 개수를 2개에서 $q$개로 일반화
      - $y^{(t)}$의 $j$의 요소가 1일 때, $\frac{\partial J^{(t)}}{\partial v_{ji}} = (y_j^{\prime(t)} - 1)h_i^{(t)}$
      - $y^{(t)}$의 $j$의 요소가 0일 때, $\frac{\partial J^{(t)}}{\partial v_{ji}} = y_j^{\prime(t)}h_i^{(t)}$
    - 따라서, $\frac{\partial J^{(t)}}{\partial v_{ji}} = (y_j^{\prime(t)} - y_j^{(t)})h_i^{(t)}$
    - $1~T$의 전체 시간을 처리하면, $\frac{\partial J^{(t)}}{\partial v_{ji}} = \displaystyle\sum_{t=1}^T{(y_j^{\prime(t)} - y_j^{(t)})h_i^{(t)}}$
  - 은닉층 미분 알고리즘
    - 은닉층에서는 $t$ 순간 은닉층의 값이 $t$와 그 이후의 은닉층, 출력층에 모두 영향을 주기 때문에 순환식으로 표현해야 한다
    - 마지막 순간 T에서, $\frac{\partial J^{(T)}}{\partial h^{(T)}} = \frac{\partial J^{(T)}}{\partial o^{(T)}}\frac{\partial o^{(T)}}{\partial h^{(T)}} = \frac{\partial J^{(T)}}{\partial o^{(T)}}\frac{\partial(Vh^{(T)} + c)}{\partial h^{(T)}} = V^T\frac{\partial J^{(T)}}{\partial o^{(T)}}$
    - $T-1$에서는 $T-1$의 출력층과 $T$의 은닉층에 영향을 끼치므로, $\frac{\partial (J^{(T-1)} + J^{(T)})}{\partial h^{(T-1)}} = \frac{\partial J^{(T-1)}}{\partial o^{(T-1)}}\frac{\partial o^{(T-1)}}{\partial h^{(T-1)}} + \frac{\partial h^{(T)}}{\partial h^{(T-1)}}\frac{\partial J^{(T)}}{\partial h^{(T)}} = V^T\frac{\partial J^{(T-1)}}{\partial o^{(T-1)}} + W^T\frac{\partial J^{(T)}}{\partial h^{(T)}}D(1-(h^{(T)})^2)$
      - 이때, $D(1-(h^{(T)})^2)$은 $i$번 열의 대각선이 $1-(h_i^{(T)})^2$을 가진 대각 행렬(diagonal matrix)이다
    - $t$에서의 식으로 일반화 하면, $\frac{\partial J^{(\tilde{t})}}{\partial h^{(t)}} = \frac{\partial (J^{(T)} + J^{(T-1)} \cdots + J^{(t)}) }{\partial h^{(t)}} = V^T\frac{\partial J^{(t)}}{\partial o^{(t)}} + W^T\frac{\partial J^{(\tilde{t+1})}}{\partial h^{(t+1)}}D(1-(h^{(t+1)})^2)$
    - ![image](https://github.com/kimho1wq/TIL/assets/15611500/caea3008-9aae-4c8b-bfd0-f25efdfacb3e)
  - BPTT 알고리즘 요약
    - $\frac{\partial J}{\partial V} = \displaystyle\sum_{t=1}^T{\frac{\partial J^{(t)}}{\partial o^{(t)}}h^{(t)^T}}$
    - $\frac{\partial J}{\partial W} = \displaystyle\sum_{t=1}^T{D(1-(h^{(t)})^2)\frac{\partial J^{(\tilde{t})}}{\partial h^{(t)}}h^{(t-1)^T}}$
    - $\frac{\partial J}{\partial U} = \displaystyle\sum_{t=1}^T{D(1-(h^{(t)})^2)\frac{\partial J^{(\tilde{t})}}{\partial h^{(t)}}x^{(t)^T}}$
    - $\frac{\partial J}{\partial c} = \displaystyle\sum_{t=1}^T{\frac{\partial J^{(t)}}{\partial o^{(t)}}}$
    - $\frac{\partial J}{\partial b} = \displaystyle\sum_{t=1}^T{D(1-(h^{(t)})^2)\frac{\partial J^{(\tilde{t})}}{\partial h^{(t)}}}$
- 장기 문맥 의존성(Long-term dependency)
  - 순차 데이터는 문맥 의존성을 가지는데, 이러한 관련 요소가 멀리 떨어져있는 경우를 장기 의존성이라고 한다
  - 긴 순차 데이터에서 그레디언트 소멸(gradient vanishing) 현상이 일어나면 장기 의존성(long-term dependency)를 제대로 반영하지 못할 수 있다
  - 또는 그레디언트가 기하급수적으로 커겨 학습을 진행할 수 없는 그레디언트 폭발(gradient explosion) 현상이 일어날 수 있다
  - RNN는 $W$를 공유하여 반복적으로 적용하기 때문에 계속 작아지거나 계속 커지는 문제가 발생할 수 있따 

### LSTM(long short term memory)
- 여러 종류의 게이트가 있어 입력을 선별적으로 허용하고 계산 결과를 선별적으로 출력할 수 있다
  - LSTM 메모리 블록의 구조와 동작(j번째 메모리 블록)
    - ![image](https://github.com/kimho1wq/TIL/assets/15611500/33581563-41f4-4fc5-a929-453aa93d3f47)
    - ![image](https://github.com/kimho1wq/TIL/assets/15611500/5afdd382-53c1-4672-bba5-cff033e7d56d)
    - ![image](https://github.com/kimho1wq/TIL/assets/15611500/e036f64e-d8ba-41f0-9681-c165fb85da70)
    - 이전 순간 $t-1$의 은닉층 상태 $h^{(t-1)}$은 입력단($w^g$), 입력 게이트($w^i$), 출력 게이트($w^o$) 3곳에 입력된다
    - 입력 벡터 $x^{(t)}$도 입력단($u^g$), 입력 게이트($u^i$), 출력 게이트($u^o$) 3곳에 입력된다
    - 입력/출력 게이트는 $\tau_f$의 활성함수를 사용하고, 주로 닫힌 상태와 열린 상태를 나타내기 위해 $[0.0, 1.0]$범위의 로지스틱 시그모이드(sigmoid)를 주로 사용한다
    - 망각 게이트는 나중에 추가되었으며, 이전 은닉층 상태와 입력 벡터를 받아 메모리 블록을 계산할 때 사용된다
    - 메모리 블록은 활성함수로 선형함수를 사용하여 자가 에지를 사용해 연산하고, 입력 게이트가 $0.0$이면, 메모리 내용이 이전과 같아져 이전 내용을 그래도 기억(이전 입력의 영향력 범위가 확장됨)하는 식으로 동작한다
  - LSTM 연산 요약
    - 입력단: $g = \tau_g(u_j^g x^{(t)} + w_j^g h^{(t-1)} + b_j^g)$
    - 입력 게이트(input gate): $i = \tau_f(u_j^i x^{(t)} + w_j^i h^{(t-1)} + b_j^i)$
    - 출력 게이트(output gate): $o = \tau_f(u_j^o x^{(t)} + w_j^o h^{(t-1)} + b_j^o)$
    - 망각 게이트(forget gate): $f = \tau_f(u_j^f x^{(t)}+ w_j^f h^{(t-1)} + b_j^f)$
    - 메모리 블록: $s^{(t)} = f \odot s^{(t-1)} + g \odot i$
    - 은닉층 상태: $h^{(t)} = \tau(s^{(t)}) \odot o$
    - 출력단: $y^{\prime(t)} = softmax(Vh^{(t) + c}$




















