# 순환 신경망

### 순환 신경망
- 순환 신경망의 구조
  - 순차 데이터를 처리하는 신경망은 다음 세 가지 기능을 갖추어야 한다
    - 시간성: 특징을 순서대로 한 번에 하나씩 입력해야 한다
    - 가변 길이: 길이가 $T$인 샘플을 처리하려면 은닉층 $T$번 나타나야 하며 $T$는 가변적이다
    - 문맥 의존성: 이전 특징 내용을 기억하고 있다가 적절한 순간에 활용해야 한다
  - RNN(recurrent neural network)은 순환 에지(recurrent edge)를 가짐으로써 시간성, 가변길이, 문맥 의존성이라는 세 가지 기능을 모두 갖춘다
    - ![image](https://github.com/kimho1wq/TIL/assets/15611500/a352c0a5-dfe8-48fc-a06e-9c36988d820a)
  - 시간 연결성을 순환식으로 표현: $h^{(t)} = f(h^{(t-1)}, x^{(t)}; \Theta)$, $\Theta$는 RNN의 가중치집합으로서 $U, V, W$에 해당된다
  - RNN의 은닉층은 직전 순간의 은닉층 $t-1$에 영향을 받으며, 이전 정보를 기억하는 역할을 수행하는 상태 변수 라고 한다
  - 입력층, 은닉층, 출력층의 노드 개수를 각각 $d, p, q$라 하면, $U$는 $p\*d$, $W$는 $p\*p$, $V$는 $q\*p$, $b$는 $p\*1$, $c$는 $q*1$ 이다
  - 가중치 공유의 장점
    - 1.학습 과정이 추정할 매개변수의 수를 획기적으로 줄여 최적화 문제를 합리적인 크기로 유지한다
    - 2.매개변수의 개수가 특징 벡터의 길이 $T$와 무관하게 일정하다
    - 3.특징이 나타나는 순간이 뒤바뀌더라도 같거나 유사한 출력을 만들 수 있다
- 순환 신경망의 동작
  - $a^{(t)} = Wh^{(t-1)} + Ux^{(t)} + b$
  - $h^{(t)} = \tau(a^{(t)})$
  - $o^{(t)} = Vh^{(t)} + c$
  - $\hat{y}^{(t)} = softmax(o^{(t)})$
- 시간 역전파 알고리즘(back-propagation through time, BPTT)
  - RNN의 목적함수 $J(\Theta) = \displaystyle\sum_{t=1}^T{J^{(t)}(\Theta)}$로, $J^{(T)}(\Theta)$는 순간 $t$에서 오류를 추정하는 함수이다
  - 

















