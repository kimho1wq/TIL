# 딥러닝 최적화

### 교차 엔트로피와 로그우도
- 딥러닝에서 평균제곱 오차(MSE)를 목적함수로 사용하면 시그모이드(sigmoid)의 도함수($\sigma^\prime$)로 인해 더딘 학습현상이 발생할 수 있다
  - $\frac{\partial e}{\partial w} = -(y-o) x \sigma^\prime (wx+b)$
  - $\frac{\partial e}{\partial b} = -(y-o) \sigma^\prime (wx+b)$
- 교체 엔트로피(cross entropy)를 기반으로 한 목적함수
  - $H(P,Q) = -\displaystyle\sum_{y \in {0,1}}{P(y)\log_{2}{Q(y)}}$ )
  - 여기서 $y$가 확률변수 역할을 하며, y는 정답 레이블을 가지므로 $y \in {0,1}$이다
  - $P$를 정답 레이블, $Q$를 신경망의 출력으로 가정한다면 다음과 같은 수식으로 통일할 수 있다
    - ![image](https://github.com/kimho1wq/TIL/assets/15611500/aa16fc6c-4788-4970-b7af-fb9b48114c50)
    - P(1) = y, Q(1) = o, $P(0) = 1-y, Q(0) = 1-o$
    - $o = \sigma(z)$, $z = wx+b$
    - $E = -(ylog_2 o + (1-y)log_2(1-o))$
      - ```
        y=1, o=0.98일 경우,
        e = -(1*log0.98 + 0*log0.02) = 0.0291
        y=1, o=0.001일 경우,
        e = -(1*log0.0001 + 0*log0.9999) = 13.2877
        ```
    - $\frac{\partial e}{\partial w} = -(\frac{y}{o}-\frac{1-y}{1-o}) \frac{\partial o}{\partial w}$
      - $= -(\frac{y}{o}-\frac{1-y}{1-o}) \frac{\partial z}{\partial w} \frac{\partial o}{\partial z} $
      - $= -(\frac{y}{o}-\frac{1-y}{1-o}) x\sigma^\prime(z)$
      - $= -(\frac{y}{o}-\frac{1-y}{1-o}) x \sigma(z)(1-\sigma(z)) $
      - $= -(\frac{y}{o}-\frac{1-y}{1-o}) xo(1-o) $
      - $= x(o-y) $
    - $\frac{\partial e}{\partial b} = (o-y)$
  - c개의 출력 노드를 가진 신경망에서의 cross entropy
    - $E = -\display\sum_{i=1,c}{(y_ilog_2 o_i + (1-y_i)log_2(1-o_i))}$
- 로그우도(log likelihood)
  - 모든 노드값을 고려하지 않으며, 샘플의 레이블에 해당하는 노드의 출력값 $o_y$ 하나만 본다
  - $E = -log_2 o_y$
    - ```
      y=2, output2=0.0508 일때, 오류값(e) = log0.0508 = 4.2990
      y=3, output3=0.8360 일때, 오류값(e) = log0.8360 = 0.2584
      ```
- softmax 활성함수
  - ![image](https://github.com/kimho1wq/TIL/assets/15611500/14e6150d-321c-4033-b811-3d4be1b622a9)
  - 출력 노드에서 주로 사용하며 softmax의 출력값을 모두 더하면 1이되고, 최대값을 더욱 활성화하고 작은 값을 더 억제하는 효과를 낼 수 있다 (max 함수에 가깝지만 더 부드럽다)
  - 학습 샘플이 알려 주는 부류에 해당하는 출력 노드값에 집중하기 때문에, 로그우도(log likelihood)와 결합하여 자주 사용된다


### 성능 향상을 위한 기법
- 표준화(standardization)
  - 특징값이 모두 양수일 경우 여러 가중치가 다같이 증가하거나 감소하면 최저점을 찾아가는 경로가 모호해져서 수렴 속도가 저하된다
  - 특징값의 규모가 다를 경우 ($x_1$이 $x_2$보다 값의 규모가 100배 정도 작다면) 규모가 작은 특징은($x_1$) 더 느리게 학습된다
  - ![image](https://github.com/kimho1wq/TIL/assets/15611500/c3539570-a861-4d8d-a09a-f80ea5ad4155)
  - 따라서 특징값의 평균이 0이 되도록 변환하고, 모든 특징의 표준편차를 1.0으로 통일하면 학습 속도를 균일하게 맞출 수 있다
    - $x_i^{new} = \frac{x_i^{old} - \mu_i}{\sigma_i}$, $x_i^{new}$는 z-score 또는 표준점수라고 한다
    - 더 중요한 특징이 존재한다면 그 특징의 표준편차를 크도록($>1.0$) 변환시킬 수 있다
  - 정규화(normalization)는 Min-max normalization으로 값의 범위를 0~1사이로 옮겨주는 것 이라고 한다
- 가중치 초기화
  - 대칭적 가중치(symmetry weight)가 발생한 경우
    - ![image](https://github.com/kimho1wq/TIL/assets/15611500/9db1a3eb-4217-48a6-ae58-f6b24f559af6)
    - $l-1$층의 두 노드 $z_1^{l-1}$값과 $z_2^{l-1}$ 값이 같아지고, 오류 역전파에서 $u^l^{11}, u^l_{12}$과 $u^l^{21}, u^l_{22}$이 같은 값으로 갱신되버리는 문제가 발생한다
    - 결국, $l-1$층의 두 노드는 같은 일을 하는 셈이 되어서 중복성 문제가 발생
  - 초기 가중치를 초기할 때, 가중치가 0에 너무 가깝다면 gradient가 작아져 학습이 매우 느려지고, 너무 크면 과잉적합(overfitting)이 발생할 수 있다
  - [Glorot2010]이 제시한 가중치 난수 초기화 방법
    - $r = \frac{\sqrt{6}}{\sqrt{n_{in} + n_{out}}}$을 결정한 후, $[-r, r]$의 범위의 난수를 생성하는 방식
    - $n_{in}, n_{out}$은 각각 노드로 들어오는 edge의 개수와 나가는 edge 개수를 의미한다
  - AlexNet은 평균이 0이고 표준편차가 0.01인 가우시언 분포(gaussian distribution)에서 생성된 난수로 가중치를 초기화하였다
  - 또한, 노드의 출력값 분포를 일정하도록(출력값의 분산이 1이 되도록) 강제화하는 방법도 있다 ([Mishjin2016])
- 모멘텀(momentum)
  - gradient에 스무딩(smoothing)을 가하면서 수렴 속도를 개선하는 방법이다
  - 이동량이 너무 커 적잘한 곳을 지나치는 오버슈팅(overshooting) 현상이 존재할 때 더욱 빠르게 최적해를 찾을 수 있다
  - 물리에서 모멘텀은 질량 x 속도 이지만, 신경망에서의 질량은 1이라 가정하여 속도를 나타내는 벡터 $v$를 사용한다
  - 처음 시작할 때는 v=0으로 놓고 시작하며, 미니배치를 사용한다면 $\frac{\partial J}{\partial \theta}$를 미니배치의 평균으로 대치한다
    - $v = \alpha v - p\frac{\partial J}{\partial \theta}$
    - $\theta = \theta + v$
    - $\alpha$는 $[0,1]$범위에서 이전 gradient에 가중치를 부여하며, $p$는 learing rate
  - 네스테로프 모멘텀(Nesterov-momentum) 기법
    - 모멘텀 기법을 개선하여 현재 $v$값을 이용하여 다음 이동할 $\tilde{\theta}$를 미리 예견(lookahead)한 후, 그곳의 gradient를 사용하는 방식
    - ![image](https://github.com/kimho1wq/TIL/assets/15611500/72afd531-156c-435e-bb00-322a3b08ba15)
    - $\tilde{\theta} = \theta + \alpha v$
    - $v = \alpha v - p\frac{\partial J}{\partial \theta}|_{\tilde{\theta}}$
    - $\theta = \theta + v$
- 적응적 학습률(adaptive learning rate)
  - k개의 매개변수( $p\frac{\partial J}{\partial \theta} = (p\frac{\partial J}{\partial \theta_1}, \cdots, p\frac{\partial J}{\partial \theta_k})^T$ )가 각각 자신의 상황에 따른 학습률(p)을 조절해 사용하도록 하는 기법
  - learning rate인 $p$가 너무 높으면 오버슈팅(overshooting)의 진자 운동이 생기고 너무 낮으면 오랜 시간이 걸린다
  - 적응적인 학습률 조절 작업을 담금질하다(annealing) 라고도 한다 
  - AdaGrad(adaptive gradient)
    - 매개변수별로 그레디언트 누적 벡터값 $r$과 갱신값 $\Delta \theta$를 계산한다
    - 이전 gradient의 누적값($r_i$)가 크면 $|\Delta \theta_i|$는 작아져서 조금만 이동하게 되며, 작으면 많이 이동하게 된다
    - $\frac{p}{\epsilon + \sqrt{r}}$이 상황에 따라 보폭을 정해 주는 정응적 학습률이 된다
    - $\epsilon$은 분모가 0이 되는 것을 방지하는 역할, $\odot$은 요소별 곱, $g = \frac{\partial J}{\partial \theta}$
    - $r=0$로 초기화한다
      - $r = r + g \odot g$
        - $(r_1, \cdots, r_k)^T = (r_1 + g_1^2, \cdots, r_k + g_k^2)^T$
      - $\Delta \theta = -\frac{p}{\epsilon + \sqrt{r}} \odot g$
        - $(\Delta \theta_1, \cdots, \Delta \theta_k)^T = (-\frac{p g_1}{\epsilon + \sqrt{r_1}}, \cdots, -\frac{p g_k}{\epsilon + \sqrt{r_k}})^T$
      - $\theta = \theta + \Delta \theta$
        - $(\theta_1, \cdots, \theta_k)^T = (\theta_1 + \Delta \theta_1, \cdots, \theta + \Delta \theta_k)^T$
    - 다만, 오래된 gradient와 최근 gradient가 알고리즘이 끝날 때까지 같은 비중의 역할을 하기 때문에, r이 점점 커져 수렴하지 못한 상황에서 p가 0에 가까워질 수도 있다
  - RMSProp
    - 오래된 gradient의 영향력을 지수적으로 줄이기 위해 가중 이동 평균(weighted moving average)기법을 적용한다
    - $\alpha$는 $0.9, 0.99, 0.999$와 같은 값을 사용한다
    - $r=0$로 초기화한다
      - $r = \alpha r + (1-\alpha) g \odot g$
      - $\Delta \theta = -\frac{p}{\epsilon + \sqrt{r}} \odot g$
      - $\theta = \theta + \Delta \theta$
  - Adam(adaptive moment)
    - RMSProp에 모멘텀 기법을 추가로 적용한 알고리즘이다
    - $r=0, v=0, t=1$로 초기화한다
      - $v = \alpha_1 v - (1-\alpha_1) g$
      - $v = \frac{1}{1-(\alpha_1)^t}v$
      - $r = \alpha_2 r + (1-\alpha_2) g \odot g$
      - $r = \frac{1}{1-(\alpha_2)^r}v$
      - $\Delta \theta = -\frac{p}{\epsilon + \sqrt{r}} \odot v$
      - $\theta = \theta + \Delta \theta$
      - $t++$
- 배치 정규화(batch normalization)















