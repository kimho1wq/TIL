# 딥러닝 기초

### 컨볼루션 신경망
- 컨볼루션 신경망(convolutional neural network, CNN)
  - 영상처리나 신호처리 분야에서 일반적으로 사용하는 컨볼루션 연산을 적용함으로써 모델의 복잡도를 낮추고 좋은 특징을 추출한다
  - 부분연결(partially-connected) 구조로 MLP에 비해 CNN은 완전연결(fully-connected) 구조로 모델의 복잡도가 훨씬 낮다 (가중치의 개수가 적음)
  - 가변크기의 입력을 처리할 수 있으며, 이웃 요소와 연관이 깊은 격자 구조를 가진 데이터에 적합하다 (MLP은 격자 구조의 순서를 섞어버림)
  - 가중치 공유(weight sharing) 기법을 사용하기 때문에 한 층에서는 오른쪽 층의 노드 개수에 따라 $h*n_l$개의 가중치가 존재한다 ($h$는 커널의 크기, 홀수)
- 컨볼루션 연산
  - 컨볼루션에 사용되는 가중치(weight)를 커널(kernel) 또는 필터(filter)라고 하며, 출력을 특징 맵(feature map)이라고 한다
  - 컨볼루션은 해당하는 요소끼리 곱하고 결과를 모두 더하는 단순한 선형 연산 ($u$는 커널, $z$는 입력, $s$는 출력)
    -  ![image](https://github.com/kimho1wq/TIL/assets/15611500/f9a22902-393d-4701-bb9b-8453adad8e9c)
    - 1차원: $s(i) = z \cdot u = \displaystyle\sum_{x=-(h-1)/2}^{(h-1)/2}{z(i+x)u(x)}$
    - 2차원: $s(j,i) = z \cdot u = \displaystyle\sum_{y=-(h-1)/2}^{(h-1)/2}\displaystyle\sum_{x=-(h-1)/2}^{(h-1)/2}{z(j+x,i+x)u(y, x)}$
    - 커널이 -(0.33, 0.34, 0.33)이면 노드 3개의 값을 평균하는 스무딩의 효과이며, (-1, 0, 1)은 수직 방향의 에지를 추출하는 효과이다
  - 일반적으로는 양쪽 가장자리에서 총 h-1개의 노드가 줄어든다 (padding이 필요)
  - 바이어스(bias) 항상 1을 입력받아서 컨볼루션 결과에 바이어스값만큼 더하는 효과를 준다 (bias 수는 출력 채널 수와 같다)
    - parameter 수 = weight 수 + bias 수 = {(입력 채널 수) x (filter height) x (filter width) + 1} x (출력 채널 수)
    - 2차원 영상이 7x7 크기의 커널을 64개 사용한다면, (1x7x7 + 1) x 64개의 매개변수를 학습시켜야 한다
- 풀링(pooling) 연산
  - 잡음 등 지나치게 상세한 정보를 포함한 특징 맵에서 요약통계(summary statistics)를 추출함으로써 성능 향상에 기여할 수 있다
  - 특징 맵(feature map)맵의 수가 그대로 유지되지만, 특징맵이 작아지므로 속도 향상과 메모리 효율에도 기여한다
  - 학습할 매개변수가 없으며, CNN layer에서 Fully-connected layer로 입력을 넘겨줄 때 원하는 크기로 조정하기 위해 사용되기도 한다
- ResNet
  - 잔류 학습(residual learning)이라는 아이디어를 활용하여 gradient vanishing problem을 완화시켜 DNN의 층수를 최대 1200층 까지 늘렸던 네트워크
    - $\odot$은 컨볼루션 연산, $\tau$는 활성함수로서 ReLU
    - $F(x) = \tau(x \bullet w_1) \bullet w_2$
    - $y = \tau(F(x) + x)$
  - residual($F(x)$)에 shortcut connection($x$)을 더했을 때의 역전파 gradient
    - $\xi$는 목적함수이고, $l, L$은 빌딩블록의 번호
    - $\frac{\partial \xi}{\partial x_l} = \frac{\partial \xi}{\partial x_L}\frac{\partial x_L}{\partial x_l} = \frac{\partial \xi}{\partial x_L}(1 + \frac{\partial}{\partial x_l}\displaystyle\sum_{i=l}^{L-1}{F(x_i)})$
    - $\frac{\partial}{\partial x_l}\displaystyle\sum_{i=l}^{L-1}{F(x_i)} = 0$이 되더라도 최소 $+1$의 gradient가 남아있기 때문에 vanishing problem이 발생하지 않는다
  - 배치 정규화(batch normalization)을 적용한 후 ReLU를 적용하여 드롭아웃 규제 기법의 효과를 발휘한 네트워크 (앙상블 효과)
  

### 생성 모델
- 생성 모델(generative model)과 분별 모델(discriminative model)의 차이
  - ![image](https://github.com/kimho1wq/TIL/assets/15611500/2dfd5069-679a-4eb4-a6d9-1c2e55527cf5)
  - Ex) 생성 모델과 분별 모델의 확률분포 추정과 예측
    - ```
      * 특징 벡터가 2차원이고, 부류가 2개라 가정한 훈련집합은 다음과 같다
      X = {(0,0),(0,0),(0,1),(0,1),(0,1),(1,0),(1,1),(1,1),(1,1),(1,1)}
      Y = {1,1,1,0,1,0,0,0,0,1}
    
      * 데이터의 확률분포를 학습한다 (이 예제는 단순하여 실제로 계산한다)
      - P(x)
      P(x=(0,0)) = 0.2, P(x=(0,1)) = 0.3, P(x=(1,0)) = 0.1, P(x=(1,1)) = 0.4
    
      - P(x|y)
      P(x=(0,0)| y=0) = 0.0, P(x=(0,0)|y=1) = 0.4,
      P(x=(0,1)| y=0) = 0.2, P(x=(0,1)|y=1) = 0.4,
      P(x=(1,0)| y=0) = 0.2, P(x=(1,0)|y=1) = 0.0,
      P(x=(1,1)| y=0) = 0.6, P(x=(1,1)|y=1) = 0.2
      
      - P(x,y)
      P(x=(0,0), y=0) = 0.0, P(x=(0,0), y=1) = 0.2,
      P(x=(0,1), y=0) = 0.1, P(x=(0,1), y=1) = 0.2,
      P(x=(1,0), y=0) = 0.1, P(x=(1,0), y=1) = 0.0,
      P(x=(1,1), y=0) = 0.3, P(x=(1,1), y=1) = 0.1,
    
      - P(y|x)
      P(y=0| x=(0,0)) = 0.0,  P(y=1| x=(0,0)) = 1.0,
      P(y=0| x=(0,1)) = 0.33, P(y=1| x=(0,1)) = 0.67,
      P(y=0| x=(1,0)) = 1.0,  P(y=1| x=(1,0)) = 0.0,
      P(y=0| x=(1,1)) = 0.75, P(y=1| x=(1,1)) = 0.25
      
      -> 생성 모델이 P(x)를 사용한다면, seed값으로 0~1의 실수 난수를 생성했을 때, 네 가지 x값의 확률에 따라 x를 생성할 수 있다
      -> 분별 모델은 테스트 샘플 x=(0,1)가 주어지면 y=0일 확률이 0.33이고, y=1일 확률이 0.67이므로 x => (y=1)로 분류할 수 있다
      ```
  - 생성 모델은 모방하고자 하는 데이터의 확률분포를 알아낼 수 있다면 새로운 샘플을 생성할 수 있다
  - 생성 모델로는 GAN(generative adversarial network), VAE(variational autoencoder), RNN, RBM 등이 있다
- GAN
  - 생성기(generator)와 분별기(discriminator)를 서로 대립시켜 학습하는 네트워크
  - 생성기 G는 가짜 샘플을 생성하고, 분별기 D는 훈련집합의 진짜 샘플과 가짜 샘플을 구별하며, G가 만들어내는 샘플을 D가 구별할 수 없을 때까지 학습시킨다
  - GAN의 구조 
    - ![image](https://github.com/kimho1wq/TIL/assets/15611500/270c7829-1d1f-40fc-a417-901a36714d57)
    - $\theta_D$는 최대화 문제로 SGA(stochastic gradient ascent)를 적용한다
      - $\theta_D = \theta_D + r\frac{\partial J_D}{\partial \theta_D}
      - $J_D(\theta_D) = log(f_D(x^{real})) + log(1-f_D(x^{fake})) = log(f_D(x^{real})) + log(1-f_D(f_G(z)))$
      - $x^{real}$에 대해 $f_D$가 0.99를 출력하고 $x^{fake}$에 대해 0.01을 출력하면 $J_D(\theta_D) \approx 0$이 되어 최고값에 가까워진다
    - $\theta_G$는 최소화 문제로 SGD(stochastic gradient descent)를 적용한다
      - $\theta_G = \theta_G - r\frac{\partial J_G}{\partial \theta_G}
      - $J_G(\theta_G) = log(1-f_D(f_G(z)))$
  - DCGAN(deep convolutional GAN)
    - GAN에 CNN을 적용한 것으로, 생성 모델이기 때문에 기존 방향과 반대방향으로 진행한다


















