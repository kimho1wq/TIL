# 비지도 학습

### 비지도 학습(unsupervised learning) 이란
- 비지도 학습(unsupervised learning)은 데이터에 내재한 구조를 잘 파악하여 새로운 정보를 발견하는 작업이다
- 비지도 학습의 일반 과업
  - 군집화: 특징 공간에서 가까이 있는 샘플을 모아 같은 그룹으로 묶는 일
  - 밀도 추정: 데이터로부터 확률분포를 추정하는 일
  - 공간 변환: 데이터가 정의된 원래 특징 공간을 저차원 공간 또는 고차원 공간으로 변환하는 일
- 비지도 학습에서 사용하는 사전 지식(prior knowledge)
  - 매니폴드 가정(manifold hypothesis): 데이터 집합은 하나의 매니폴드 또는 여러 개의 매니폴드를 구성하며, 모든 샘플은 매니폴드와 가까운 곳에 있다
  - 매끄러움 가정(smoothness hypothesis): 샘플을 다른 외부 요인에 따라 변화하지만, 매끄러운 곡면을 따라 위치가 변한다


### 군집화
- 군집화란 훈련집합 $X$가 주어지면, 다음 식을 만족하는 군집집합 $C$를 찾아내는 작업이다
- 군집의 개수($k$)를 부류의 개수로 간주할 수 있으므로 군집화를 부류 발견(class discovery) 작업이라고도 한다
  - $c_i \neq \emptyset$, $(i=1,2, \cdots, k)$
  - $\displaystyle\bigcup^k_{i=1}c_i = X$
  - $c_i\bigcap c_j = \emptyset$, $(i \neq j)$
- k-means 알고리즘
  - 군집의 개수($k$)를 지정해야 한다
  - 군집 중심을 갱신하는 방법에는 샘플의 평균으로 중심을 갱신하는 방법(k-means)과 샘플 중에서 다른 샘플까지 거리의 합이 최소가 되는 샘플을 대표로 뽑는 방법(k-medoids)이 있다
    - ```
      k개의 군집 중심 Z={z1,z2,...,zk}를 초기화 한다
      while(true)
        for (i=1 to n)
          xi를 가장 가까운 군집 중심에 배정한다
        if (xi 군집 배정이 이전 루프 배정과 같으면) break
        for (j=1 to k)
          zj에 배정된 샘플의 평균으로 zj를 대치한다
      for (j=1 to k)
        zj에 배정된 샘플을 cj에 대입한다
      ```
  - k-means의 최적화 목적함수
    - $Z$는 군집 중심이고, $A$는 샘플의 배정 정보를 나타내는 $k*n$ 행렬이다
    - $k*n$행렬은 $i$번째 샘플이 $j$번째 군집에 배정되었다면 $a_{ji}=1$, 그렇지 않으면 $0$이다
    - $J(Z,A) = \displaystyle\sum^n_{i=1}\displaystyle\sum^k_{j=1}{a_{ji}dist(x_i,z_j)}$
    - ![image](https://github.com/kimho1wq/TIL/assets/15611500/19690686-6db2-4b1d-846f-da93f3b98924)
  - k-means는 목적함수의 값이 작아지는 방향으로 해를 갱신하며, 어떤 초기 군집 중심을 가지고 출발하더라도 반드시 수렴한다는 성질이 증명되어있다
  - 다만 초기 군집 중심이 달라지면 최종 결과가 달라지는 문제가 존재한다
  - 다중 시작(multi-start) k-means 알고리즘
    - k-means 알고리즘이 초기 군집 중심에 민감하다는 문제를 해결하기 위해서, 서로 다른 초기 군집 중심을 가지고 k-means를 수행한 다음, 가장 좋은 품질의 해를 선택하는 전략이다
  - EM(expectation maximization) 알고리즘
    - ![image](https://github.com/kimho1wq/TIL/assets/15611500/1272e061-e4c6-475a-a6fb-2d66a312dadd)
    - 은닉변수(latent variable)와 매개변수 추정을 번갈아 수행하면서 최적의 해를 찾는 과정을 의미한다
    - 은닉변수를 추정하는 단계를 E(expectation)단계, 매개 변수를 추정하는 단계를 M(maximization)단계라고 한다
- 친밀도 전파 알고리즘
  - 샘플 간의 유사도(similarity)로 부터 책임(responsibility) 행렬 $R$과 가용(availability) 행렬 $A$라는 두 종류의 친밀도(affinity) 행렬을 계산하고, 이 친밀도 정보를 이용하여 군집을 찾는 알고리즘이다
  - 군집 개수를 미리 지정할 필요가 없으며, 자가 유사도(self-similarity, $s_{kk}$)를 설정하여 군집 개수를 조절할 수 있다
  - $s_{kk}$를 최솟값으로 설정하면 군집이 적게 생성되며, 최댓값으로 설정하면 군집이 아주 많이 생성되는 하이퍼 매개변수이다
    - ![image](https://github.com/kimho1wq/TIL/assets/15611500/d7f77927-e93c-42eb-9f84-6db883af3001)
  - 책임 행렬 $R$
    - $r_{ik} = s_{ik} - max_{k^\prime \neq k}(a_{ik^\prime} + s_{ik^\prime})$
    - $s_{ik} = -||x_i - x_k||^2_2$, ($i \neq k$이고, $i,k = 1,2,\cdots,n$)
      - 두 샘플 $i$와 $k$의 유사도 $s_{ik}$는 유클리디언 거리의 제곱에 음수를 취했으므로 가까울수록 큰 값을 가진다
      - $r_{kk}$와 $a_{kk}$는 자가 친밀도(self-affinity) 라고 한다
      - $i$와 $k$가 친밀할수록(유사할수록) 큰 값을 부여하고, $i$가 $k$ 이외의 다른 샘플과 더 친밀할수록 더 큰값을 뺀다
  - 가용 행렬 $A$
    - $a_{ik} = min(0, r_{kk} + \displaystyle\sum_{i^\prime \neq i,k}{max(0,r_{i^\prime k})})$, $(i \neq k)$
      - 다른 샘플($i^\prime$)과 친밀도가 약한 경우 $a_{ik}$가 작아지고, 다른 샘플과 친밀도가 강한 경우 $a_{ik}$가 커진다
      - $r_{kk}$는 샘플 $k$가 자신에게 대표 자격을 부여하는 값으로 해석되며, 이 값이 클수록 $a_{ik}$가 커진다
      - $a_{ik}$는 0을 임곗값으로 사용하며 0을 넘지 않도록 제어한다
    - ```
      for (모든 샘플 쌍 i와 k에 대하여)
        if(i!=k) sik = -||xi - ik||_2^2 //샘플 간의 유사도를 계산한다
      for (k=1 to n)
        skk = 최솟값, 최댓값 또는 중앙값 (hyper-parameter) // 자가 유사도를 설정한다
      for (모든 샘플 쌍 i와 k에 대하여)
        aik = 0, rik = 0
      repeat
        for (모든 샘플 쌍 i와 k에 대하여)
          rik = sik - max(aik + sik)
          // 같은 값을 중심으로 커졌다 작아졌다를 반복하는 현상을 반복하기 위해 damping factor(lambda=0.5)를 적용한다
          rik = lambda * rik + (1-lambda) * rik
        for (모든 샘플 쌍 i와 k에 대하여)
          // 친밀도 행렬로부터 군집 정보를 추출한다 (자가 친밀도가 높을수록 군집 대표의 가능성이 크다)
          if (i != k)
            aik = min(0, rkk + sum(max(0, rik))
          else
            akk = sum(max(0, rik))
      until(멈춤 조건)
      for (i=1 to n)
        k = argmax(aik + rik)
        // a(i,i) + r(i,i)가 (a(i,j) + r(i,j), (i!=j)) 보다 크면 샘플 xi를 군집 대표로 취한다
        if (k = i)
          zi = i (이 샘플은 군집의 중심)
        else
          zi = k  (이 샘플은 k 군집 중심에 속함)
      ```

### 밀도 추청
- 어떤 점 $x$에서 데이터가 발생할 확률인 확률밀도함수 $P(x)$를 구하는 문제가 밀도 추정(density estimation)이다
- 커널 밀도 추정
  - 히스토그램 방법(histogram method)
    - 각 차원을 여러 구간으로 나누어 특징 공간의 칸(bin)을 집합으로 분할한 다음, 각각의 칸에 있는 샘플의 빈도를 세는 것
    - $P(x) = \frac{bin(x)}{n}$, $bin(x)$는 점 $x$가 놓인 칸에 있는 샘플의 개수, $n$은 훈련집합에 있는 샘플의 수이다
  - 커널 밀도 추정법(kernel density estimation method)
    - ![image](https://github.com/kimho1wq/TIL/assets/15611500/351814f2-b08a-44ca-850a-553177555d03)
    - $K$는 표준커널함수이고, $K_h$는 크기 변환된 커널함수, $P_h$는 확률밀도함수를 나타낸다
    - $h$는 대역폭(bandwidth)으로 하이퍼 매개변수로, $h<1$이 되면 좁고 높은 모양의 커널이 되며 $h>1$이 되면 넓고 낮은 커널이 된다
    - $P_h(x) = \frac{1}{n}\displaystyle\sum_{i=1}^n{K_h(x-x_i)} = \frac{1}{nh^d}\displaystyle\sum_{i=1}^n{K(\frac{x-x_i}{h})}$, 여기서 $K_h(x) = \frac{1}{h^d}K(\frac{x}{h})$
    - 대역폭이 확률밀도함수 추정에 미치는 영향
      - ![image](https://github.com/kimho1wq/TIL/assets/15611500/81747ad3-668a-4e53-9661-12872c314af1)
      - 빨간색은 $h=0.05$, 녹색은 $h=2.0$, 검은색은 $h=0.337$
    - AMISE(asymptotic mean integrated squared error)등 다양한 커널의 대역폭 자동 설정 기법이 존재한다
    - 매개변수로 정의되는 일정한 모양의 함수를 사용하지 않으므로 비모수 방법(non-parametric method)이다
  - 커널 밀도 추정법의 문제점
    - 훈련집합의 샘플을 모두 저장하고 있어야 하는 메모리 기반의 방법이다
    - 새로운 샘플이 주어질 때마다 확률밀도함수($P_h$)를 다시 계산해야 하며, $\theta(nd)$만큼의 계산 시간이 소요된다 ($d$는 데이터의 차원 수, $n$은 샘플의 수)
    - 특징 공간이 고차원일수록 데이터가 희소한 문제가 생기므로, 데이터의 차원이 낮은 경우에만 사용하는 방법이다
- 가우시안 혼합
  - 데이터가 일정한 모양의 분포(Gaussian distribution 등)를 따른다는 가정하에 확률분포를 추정하는 방법
    - $P(x) = N(x;\mu, \Sigma) = \frac{1}{\sqrt{|\Sigma|}\sqrt{(2\pi)^d}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))$
    - $\mu = \frac{1}{n}\displaystyle\sum_{i=1,n}{x_i}$이고, $\Sigma=\frac{1}{n}\displaystyle\sum_{i=1,n}{(x_i-\mu)(x_i-\mu)^T}$
  - 가우시안을 이용하는 방법은 몇 개의 매개변수로 확률분포를 정의하는 모수적 방법(parametric method)이다 
  - 가우시안을 활용하면 평균 벡터 $\mu$와 공분산 행렬 $\Sigma$를 계산하면 훈련집합이 없어도 확률분포를 계산할 수 있다
  - 데이터가 $d$차원이라면 $\mu$는 $d$개, $\Sigma$는 $d^2$개의 요소를 가지므로, 총 $d+d^2$개의 값만 저장하면 된다
  - 확률밀도함수 $P(x)$는 가우시안 $k$개의 선형 결합으로 표현할 수 있다
    - $P(x) = \displaystyle\sum_{j=1}^k{\pi_j N(x; \mu_j, \Sigma_j)}$
    - 혼합 계수 $\pi_j$는 0과 1사이의 값이고, $\displaystyle\sum_{j=1}^k{\pi_j} = 1$을 만족하며, 가우시안의 갯수 $k$는 사용자가 설정한다고 가정한다
    - 추정해야 할 매개변수집합: $(\pi_1, \pi_2, \cdots, \pi_k), (\mu_1, \Sigma_1), \cdots, (\mu_k, \Sigma_k)$
  - 최대 우도를 이용한 최적화
    - $P(X|\Theta) = \displaystyle\prod_{i=1}^n{P(x_i|\Theta)} = \displaystyle\prod_{i=1}^n{(\displaystyle\sum_{j=1}^k{\pi_j N(x; \mu_j, \Sigma_j)})}$
    - $\log{P(X|\Theta)} = \displaystyle\sum_{i=1}^n{\log{(\displaystyle\sum_{j=1}^k{\pi_j N(x; \mu_j, \Sigma_j)})}}$
    - 주어진 $X$가 발생할 가능성이 가장 큰 $\Theta$를 찾기 위해 EM 알고리즘을 사용
      - 가우시안이 자신에 속하는 샘플을 알면, 소속된 샘플을 이용하여 $\mu$와 $\Sigma$를 계산하고, 계산된 매개변수로 가우시안에 소속된 샘플의 정보를 개선할 수 있다
      - 샘플 $x_i$가 $j$번째 가우시안에 속할 확률을 $z_{ji}$라 하면, 이 확률 정보를 $k*n$(가우시안 개수 x 훈련집합 수) 크기의 행렬 $Z$에 저장할 수 있다
      - E단계
        - $z_{ji} = \frac{\pi_j N(x_i)}{\displaystyle\sum_{q=1}^k{pi_q N(x_i)}}$
      - M단계
        - $\mu_j = \frac{1}{n_j}\displaystyle\sum_{i=1}^n{z_{ji}x_i}$, $n_j = \displaystyle\sum_{i=1}^n{z_{ji}}$
        - $\Sigma_j = \frac{1}{n_j}\displaystyle\sum_{i=1}^n{z_{ji}(x_i-\mu_j)(x_i-\mu_j)^T}$, $pi_j = \frac{n_j}{n}$


### 선형 인자 모델
- 선형 인자 모델(linear factor model)은 선형 연산을 이용하여 관찰한 데이터를 관찰되지 않는 변수(인자)로 변환하는 방법이다
  - 선형 인자 모델은 선형 연산을 사용하므로, 행렬 곱으로 인코딩과 디코딩 과정을 표현할 수 있다 ($q$는 변환 차원 $d$는 기존 차원)
    - 인코딩: $f: z=W_{enc}x + \alpha_{enc}$ 이고 $W_{enc}$는 $q*d$, $\alpha_{enc}$는 $q\*1$
    - 디코딩: $g: x=W_{dec}z + \alpha_{dec}$ 이고 $W_{dec}$는 $d*q$, $\alpha_{dec}$는 $d\*1$
    - $\alpha$는 데이터를 원점으로 이동하거나 잡음을 추가하는 등의 역할을 수행한다
- 주성분 분석(principal component analysis, PCA)
  - 데이터를 원점 중심으로 옮겨 놓는 일부터 시작 $(x_i = x_i - \frac{1}{n}\displaystyle\sum_{i=1}^n{x_i})$
  - $z = W^Tx$가 수행되어 $d$차원의 $x$가 $q$차원의 $z$로 변환된다 (PCA에서는 $\alpha$를 사용하지 않음)
  - 
  - 학습 알고리즘
    - PCA를 진행할 때 정보 손실을 최소화하면서 고차원 정보를 저차원으로 변환하기 위한 변환 행렬 $W$($q$개의 벡터)를 찾는 알고리즘
    - 변환된 새로운 공간에서 분산이 클수록(샘플들이 모여있을 수록) 정보 손실이 작다고 판단한다
    - $u$가 단위 벡터야 하는 조건 $u^Tu = 1$를 이용하여 라그랑주 함수 $L(u) = u^T\Sigma u + \lambda(1-u^Tu)$를 최대로 하는 $u$를 찾으면 된다
      - $\Sigma = \frac{1}{n}\displaystyle\sum_{i=1}^{n}{(x_{i}-\mu)(x_{i}-\mu)^{T}}$
      - $\frac{\partial L(u)}{\partial u} = 2\Sigma u - 2\lambda u = 0$을 만족하는 조건에서 $u$를 최대로 만들 수 있다
      - $\Sigma u = \lambda u$에서 $u$는 공분산 행렬(dxd) $\Sigma$의 고유벡터(eigen vector)이고, $\lambda$는 고윳값(eigen value)이므로, $d$개의 고유벡터($u$)와 고윳값($\lambda$)를 얻을 수 있다
    - 여기서 고유벡터(eigen vector) $u$를 주성분(principal component)라고 하고, $u$들은 서로 수직이며, 고윳값(eigen value)의 값이 클 수록 많은 정보를 유지한다
    - A가 정사각행렬에서 $det(A-\lambda I)=0$을 만족하는 방정식을 풀어서 고유값($\lambda$)를 구하고, 식에 대입하여 고유벡터($u$)를 구할 수 있다
  - 상위에 있는 몇개의 고유 벡터가 대부분의 정보를 간직하므로 보통 $q < d$로 설정하여 원래보다 낮은 차원으로 변환시킨다
- 독립 성분 분석(independent component analysis, ICA)
  - 주로 혼합 신호로부터 원래 신호를 복원하는 블라인드 원음 분리(blind source separation) 문제를 해결하기 위한 기법이다
  - 블라인드 원음 분리(blind source separation)
    - 원래 신호는 $z_1(t), z_2(t)$, 혼합 신호는 $x_1(t), x_2(t)$, $t$는 신호를 샘플링한 순간, t번째 수집된 샘플 $x_t = (x_1(t)x2(t))^T, z_t = (z_1(t)z_2(t))$
    - 혼합 신호 $x$는 원래 신호 $z$의 선형 결합으로 표현할 수 있다 $(x_1 = a_{11}z_1 + a_{12}z_2, x_2 = a_{21}z_1 + a_{22}z_2)$
    - 블라인드 원음 분리 문제는 $x = Az$에서 $A$ 행렬을 구하여 $\hat{z} = A^{-1}x$ 로 $z$를 구할 수 있다 
  - 독립성 가정
    - ICA는 적용할 수 있는 응용 범위를 넓게하기 위해 독립성 조건을 사용한다
    - $P(z) = P(z_1, z_2, \cdots, z_d) = \displaystyle\prod_{j=1}^d{P(z_j)}$
  - 비가우시안(non-Gaussian) 가정
    - 원래 신호 $z_i$가 비가우시안이어야 한다는 가정으로, $z_i$의 선형결합으로 만들어진 $x_j$가 가우시안 분포를 따른다면 $P(x)$의 결합 분포도 가우시안 분포로 원래 신호 복원이 어렵다
    - 
  - 학습 알고리즘
    - 훈련집합의 평균이 $0$ 벡터가 되도록 이동 변환한 후, 화이트닝 변환을 적용하는 전처리 과정을 먼저 수행한다
    - 화이트닝 변환의 전처리
      - $x^{\prime}_i = (D^{-\frac{1}{2}}V^T)x_i$, $(i=1,2,\cdots,n)$
      - $V$는 $X$의 공분산 행렬의 고유 벡터를 열에 배치한 $d*d$ 행렬이고, $D$는 고유값을 대각선에 배치한 $d\*d$ 대각 행렬이다
    - 최적의 매개변수 $\hat{w_j}$를 구하기 위한 식
      - $\hat{w_j} = argmax_{w_j}\check{G}(z_j)$
      - $\check{G}$는 비가우시안 정도를 측정하는 함수로, 첨도($kurtosis$)를 주로 사용한다
        - $kurtosis(z_j) = \frac{1}{n}\displaystyle\sum_{i=1}^n{z^4_{ji}}-3(\frac{1}{n}\displaystyle\sum_{i=1}^n{z^2_{ji}})^2$
        - 4차 모멘트까지 사용하며, 가우시안을 기준으로 분포가 뾰족해질수록 첨도가 커지고 펑퍼짐해지면 음수가 되고, 가우시안의 첨도는 0이다
        - 
      - $w_j$를 임의값으로 초기화한 후, 첨도를 구하고 적절한 방법으로 첨도가 작아지는 방향으로 $w_j$를 개선하면서 알고리즘을 만들 수 있다
- 희소 코딩(sparse coding)
  - 여러 개의 기저 벡터(basis vector)의 선형 결합으로 표현하는 것이 희소 코딩(sparse coding)이며, 푸리에 변환(fourier transform)이나 웨이블릿 변환(wavelet transform) 기법 등이 있다
    - $x = Da$, 이때 $D = (d_1, d_2, \cdots, d_m)$, $d_i$는 i번째 기저 벡터, 벡터 $a$는 희소 코드(sparse code)
    - $D$는 $d*m$ 행렬이고, $d$는 벡터 $x$의 차원 수 이며, D를 사전(dictionary)라고 하고, 그 요소인 $d_i$를 사전의 요소라고 한다
  - 희소 코딩에서는 비지도 학습이 훈련집합 $X$를 이용하여 자동으로 사전(dictionary)를 알아낸다
  - 기저 벡터의 크기 $d$보다 사전의 크기 $m$을 훨씬 크게 측정하고, 희소 코드(sparse code)를 구성하는 계수 대부분이 0인 특성이 있다
  - 학습 알고리즘
    - 최적의 사전($D$)과 최적의 희소 코드 $m*n$ 행렬($A$)를 알아내는 문제를 풀어야 한다
    - $\hat{D}, \hat{A} = argmin_{D,A}\displaystyle\sum^n_{i=1}{||x_i-Da_i||^2_2 + \lambda \phi(a_i)}$
    - 첫 번째 항은 희소 코드로 근사 표현되는 값 $Da_i$가 주어진 신호 $x_i$와 가까워야 한다는 조건이다
    - 두 번째 항 $\phi(a_i)$는 규제 항으로 희소 코드가 희소해야 한다는 조건이고, $\lambda$는 어느 항을 중시할지 나타내는 하이퍼 매개변수이다
    - $\phi(a_i)$은 L0 놈을 사용하면 $a_i$에서 0이 아닌 요소의 개수를 세므로 희소성을 제대로 측정하지만, 미분이 불가능하여 보통 L1 놈인 $||a_i||_1$

### 오토인코더
- 오토인코더(autoencoder)는 특징 벡터 $x$를 입력받아 동일하거나 유사한 벡터 $x^\prime$를 출력하는 신경망이다
  - 
  - 오토인코더는 레이블 정보가 없는 비지도 학습에 속하고, 보통 은닉층의 노드 개수 $m$d을 입력층의 노드 개수 $d$보다 작게 만들어 차원 축소나 압축, 분류 등의 다양한 분야에서 활용한다
  - 오토인코더에서 $h = f(x) = Wx$, $x = g(h) = Vh$의 선형 매핑을 사용하고 $m < d$이며, 목적함수로 $\hat{\Theta} = argmin_{W,V}\displaystyle\sum_{i=1}^n{L(x_i, V(Wx_i))}$를 사용한다면, 가중치의 값이 PCA가 찾아내는 주성분과 같다는 정리가 있다
- 규제 오토인코더(regularized autoencoder)
  - 규제 오토인코더는 적절한 규제 기법을 적용하는 오토인코더 모델로 SAE, DAE, CAE 등이 있다
  - SAE(sparse autoencoder)는 은닉층의 출력 $h$에 관심을 두며, $h$가 희소하도록 강제함으로써 규제 효과를 거둔다
    - 잠복 벡터 $h$는 분류를 위한 특징 벡터로 주로 사용한다
    - $SAE: \hat{\Theta} = argmin_{\Theta}\displaystyle\sum_{i=1}^n{L(x_i, V(Wx_i))} + \lambda \phi(h_i)$
    - 여기서 $\lambda \phi(h_i)$는 벡터 $h_i$가 0이 아닌 요소를 희소하게 가지도록 규제하는 항이다
  - DAE(denoising autoencoder)는 일부러 잡음을 추가한 후 잡음이 없는 원래 패턴을 찾는 방식을 주로 사용한다
    - 오토인코더는 잡음이 섞인 입력 $\tilde{x_i}$에서 원래 특징 벡터 $x_i$를 복구하여 상수로 유지되도록 강제한다
    - $DAE: \hat{\Theta} = argmin_{\Theta}\displaystyle\sum_{i=1}^n{L(x_i, V(W\tilde{x_i}))} + \lambda \phi(h_i)$
  - CAE(contractive autoencoder)
    - 인코더(encoder) 함수 $f(x)$에 관심을 두어 $f$의 야코비안(Jaconian) 행렬의 프로베니우스 놈(Frobenius norm)을 작게 유지함으로써 규제 효과를 거둔다
    - $CAE: \hat{\Theta} = argmin_{\Theta}\displaystyle\sum_{i=1}^n{L(x_i, V(Wx_i))} + \lambda \phi(x_i, h_i)$, 이때 $\phi(x_i, h_i) = {||\frac{\partial f}{\partial x}||}^2_F$
    - 은닉 노드는 0또는 1에 가까워지고 공간이 축소(contractive)되는 효과가 나타난다
- 적층 오토인코더(stacked autoencoder)
  - 오토인코더를 여러 개의 은닉층을 쌓아 확장한 구조
  - 적층 오토인코더의 학습 과정
    - 
    - 1.훈련집합에 있는 샘플로 오토인코더를 학습하고, 학습을 마치면 디코딩 부분을 때어내고 인코딩 부분만 남기며 인코딩 함수 $f_1$을 고정시킨다
    - 2.두 번째 오토인코더는 $h_1$을 입력하여 학습하고, 학습을 마치면 인코딩 부분만 남기며 $f_2$를 고정시킨다
    - 3.같은 방식으로 인코딩 함수 $f_3$를 완성하면 적층 오토인코더가 완성된다, 탐욕 층별 예비학습(greedy layer-wise pretraining)의 과정
    - 4.훈련집합 $x$의 특징 벡터인 $h_3$을 이용하여 MLP를 학습시키고, 미세 조정(fine tuning)을 이용하여 신경망 전체를 한꺼번에 학습시킨다
- 확률 오토인코더
  - 결정론적 오토인코더의 중간 과정에 확률 과정을 추가하여 확률 모델로 확장한 오토인코더
  - 확률 모델에서는 인코딩 과정의 $h=f(x)$가 $P_{encode}(h|x)$로, 디코딩 단계의 $x = g(h)$가 $P_{decode}(x|h)$로 대치된다
  - 가장 대표적인 확률 오토인코더로는 RBM(restricted Bolzmann machine)이 있으며, RBM을 여러 층으로 적층한 DBN(deep belief network)가 존재한다
  

### 매니폴드 학습
- 매니폴드(manifold)는 고차원 공간에 내재한 저차원 공간을 의미하고 주로 비선형 구조를 가지며, 매니폴드 학습은 데이터 분포의 비선형 구조를 직접적으로 고려하는 방법이다
- 매니폴드 가정(manifold hypothesis)
  - 실제 데이터는 고차원($d$) 입력 공간에 무작위로 분포하지 않고, $d$보다 낮은 차원인 매니폴드(manifold) 인근에 집중되어 있다는 가정
  - 
    - (a)그림은 데이터가 3차원에 분포하는데, 데이터 구조가 평면에 가깝다
    - (b)그림은 데이터가 3차원에 분포하는데, 돌돌말린 구조의 비선형 곡선과 파란색 직선의 2차원의 매니폴드(manifold)로 표현할 수 있다
- IsoMap
  - 최근접 이웃 그래프(nearest neighbor graph)를 구축


















