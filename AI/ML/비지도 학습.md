# 비지도 학습

### 비지도 학습(unsupervised learning) 이란
- 비지도 학습(unsupervised learning)은 데이터에 내재한 구조를 잘 파악하여 새로운 정보를 발견하는 작업이다
- 비지도 학습의 일반 과업
  - 군집화: 특징 공간에서 가까이 있는 샘플을 모아 같은 그룹으로 묶는 일
  - 밀도 추정: 데이터로부터 확률분포를 추정하는 일
  - 공간 변환: 데이터가 정의된 원래 특징 공간을 저차원 공간 또는 고차원 공간으로 변환하는 일
- 비지도 학습에서 사용하는 사전 지식(prior knowledge)
  - 매니폴드 가정(manifold hypothesis): 데이터 집합은 하나의 매니폴드 또는 여러 개의 매니폴드를 구성하며, 모든 샘플은 매니폴드와 가까운 곳에 있다
  - 매끄러움 가정(smoothness hypothesis): 샘플을 다른 외부 요인에 따라 변화하지만, 매끄러운 곡면을 따라 위치가 변한다


### 군집화
- 군집화란 훈련집합 $X$가 주어지면, 다음 식을 만족하는 군집집합 $C$를 찾아내는 작업이다
- 군집의 개수($k$)를 부류의 개수로 간주할 수 있으므로 군집화를 부류 발견(class discovery) 작업이라고도 한다
  - $c_i \neq \emptyset$, $(i=1,2, \cdots, k)$
  - $\displaystyle\bigcup^k_{i=1}c_i = X$
  - $c_i\bigcap c_j = \emptyset$, $(i \neq j)$
- k-means 알고리즘
  - 군집의 개수($k$)를 지정해야 한다
  - 군집 중심을 갱신하는 방법에는 샘플의 평균으로 중심을 갱신하는 방법(k-means)과 샘플 중에서 다른 샘플까지 거리의 합이 최소가 되는 샘플을 대표로 뽑는 방법(k-medoids)이 있다
    - ```
      k개의 군집 중심 Z={z1,z2,...,zk}를 초기화 한다
      while(true)
        for (i=1 to n)
          xi를 가장 가까운 군집 중심에 배정한다
        if (xi 군집 배정이 이전 루프 배정과 같으면) break
        for (j=1 to k)
          zj에 배정된 샘플의 평균으로 zj를 대치한다
      for (j=1 to k)
        zj에 배정된 샘플을 cj에 대입한다
      ```
  - k-means의 최적화 목적함수
    - $Z$는 군집 중심이고, $A$는 샘플의 배정 정보를 나타내는 $k*n$ 행렬이다
    - $k*n$행렬은 $i$번째 샘플이 $j$번째 군집에 배정되었다면 $a_{ji}=1$, 그렇지 않으면 $0$이다
    - $J(Z,A) = \displaystyle\sum^n_{i=1}\displaystyle\sum^k_{j=1}{a_{ji}dist(x_i,z_j)}$
    - 
  - k-means는 목적함수의 값이 작아지는 방향으로 해를 갱신하며, 어떤 초기 군집 중심을 가지고 출발하더라도 반드시 수렴한다는 성질이 증명되어있다
  - 다만 초기 군집 중심이 달라지면 최종 결과가 달라지는 문제가 존재한다
  - 다중 시작(multi-start) k-means 알고리즘
    - k-means 알고리즘이 초기 군집 중심에 민감하다는 문제를 해결하기 위해서, 서로 다른 초기 군집 중심을 가지고 k-means를 수행한 다음, 가장 좋은 품질의 해를 선택하는 전략이다
  - EM(expectation maximization) 알고리즘
    -
    - 은닉변수(latent variable)와 매개변수 추정을 번갈아 수행하면서 최적의 해를 찾는 과정을 의미한다
    - 은닉변수를 추정하는 단계를 E(expectation)단계, 매개 변수를 추정하는 단계를 M(maximization)단계라고 한다
- 친밀도 전파 알고리즘
  - 샘플 간의 유사도(similarity)로 부터 책임(responsibility) 행렬 $R$과 가용(availability) 행렬 $A$라는 두 종류의 친밀도(affinity) 행렬을 계산하고, 이 친밀도 정보를 이용하여 군집을 찾는 알고리즘이다
  - 군집 개수를 미리 지정할 필요가 없으며, 자가 유사도(self-similarity, $s_{kk}$)를 설정하여 군집 개수를 조절할 수 있다
  - $s_{kk}$를 최솟값으로 설정하면 군집이 적게 생성되며, 최댓값으로 설정하면 군집이 아주 많이 생성되는 하이퍼 매개변수이다
  - 책임 행렬 $R$
    - $r_{ik} = s_{ik} - max_{k^\prime \neq k}(a_{ik^\prime} + s_{ik^\prime})$
    - $s_{ik} = -||x_i - x_k||^2_2$, ($i \neq k$이고, $i,k = 1,2,\cdots,n$)
      - 두 샘플 $i$와 $k$의 유사도 $s_{ik}$는 유클리디언 거리의 제곱에 음수를 취했으므로 가까울수록 큰 값을 가진다
      - $r_{kk}$와 $a_{kk}$는 자가 친밀도(self-affinity) 라고 한다
      - $i$와 $k$가 친밀할수록(유사할수록) 큰 값을 부여하고, $i$가 $k$ 이외의 다른 샘플과 더 친밀할수록 더 큰값을 뺀다
  - 가용 행렬 $A$
    - $a_{ik} = min(0, r_{kk} + \displaystyle\sum_{i^\prime \neq i,k}{max(0,r_{i^\prime k})})$, $(i \neq k)$
      - 다른 샘플($i^\prime$)과 친밀도가 약한 경우 $a_{ik}$가 작아지고, 다른 샘플과 친밀도가 강한 경우 $a_{ik}$가 커진다
      - $r_{kk}$는 샘플 $k$가 자신에게 대표 자격을 부여하는 값으로 해석되며, 이 값이 클수록 $a_{ik}$가 커진다
      - $a_{ik}$는 0을 임곗값으로 사용하며 0을 넘지 않도록 제어한다
    - ```
      for (모든 샘플 쌍 i와 k에 대하여)
        if(i!=k) sik = -||xi - ik||_2^2 //샘플 간의 유사도를 계산한다
      for (k=1 to n)
        skk = 최솟값, 최댓값 또는 중앙값 (hyper-parameter) // 자가 유사도를 설정한다
      for (모든 샘플 쌍 i와 k에 대하여)
        aik = 0, rik = 0
      repeat
        for (모든 샘플 쌍 i와 k에 대하여)
          rik = sik - max(aik + sik)
          // 같은 값을 중심으로 커졌다 작아졌다를 반복하는 현상을 반복하기 위해 damping factor(lambda=0.5)를 적용한다
          rik = lambda * rik + (1-lambda) * rik
        for (모든 샘플 쌍 i와 k에 대하여)
          // 친밀도 행렬로부터 군집 정보를 추출한다 (자가 친밀도가 높을수록 군집 대표의 가능성이 크다)
          if (i != k)
            aik = min(0, rkk + sum(max(0, rik))
          else
            akk = sum(max(0, rik))
      until(멈춤 조건)
      for (i=1 to n)
        k = argmax(aik + rik)
        // a(i,i) + r(i,i)가 (a(i,j) + r(i,j), (i!=j)) 보다 크면 샘플 xi를 군집 대표로 취한다
        if (k = i)
          zi = i (이 샘플은 군집의 중심)
        else
          zi = k  (이 샘플은 k 군집 중심에 속함)
      ```

### 밀도 추청
- 어떤 점 $x$에서 데이터가 발생할 확률인 확률밀도함수 $P(x)$를 구하는 문제가 밀도 추정(density estimation)이다
- 커널 밀도 추정
  - 히스토그램 방법(histogram method)
    - 각 차원을 여러 구간으로 나누어 특징 공간의 칸(bin)을 집합으로 분할한 다음, 각각의 칸에 있는 샘플의 빈도를 세는 것
    - $P(x) = \frac{bin(x)}{n}$, $bin(x)$는 점 $x$가 놓인 칸에 있는 샘플의 개수, $n$은 훈련집합에 있는 샘플의 수이다
  - 커널 밀도 추정법(kernel density estimation method)
    - $K$는 표준커널함수이고, $K_h$는 크기 변환된 커널함수, $P_h$는 확률밀도함수를 나타낸다
    - $h$는 대역폭(bandwidth)으로 하이퍼 매개변수로, $h<1$이 되면 좁고 높은 모양의 커널이 되며 $h>1$이 되면 넓고 낮은 커널이 된다
    - $P_h(x) = \frac{1}{n}\displaystyle\sum_{i=1}^n{K_h(x-x_i)} = \frac{1}{nh^d}\displaystyle\sum_{i=1}^n{K(\frac{x-x_i}{h})}$, 여기서 $K_h(x) = \frac{1}{h^d}K(\frac{x}{h})$
    - 대역폭이 확률밀도함수 추정에 미치는 영향
      - 빨간색은 $h=0.05$, 녹색은 $h=2.0$, 검은색은 $h=0.337$
      - 
    - AMISE(asymptotic mean integrated squared error)등 다양한 커널의 대역폭 자동 설정 기법이 존재한다
    - 매개변수로 정의되는 일정한 모양의 함수를 사용하지 않으므로 비모수 방법(non-parametric method)이다
  - 커널 밀도 추정법의 문제점
    - 훈련집합의 샘플을 모두 저장하고 있어야 하는 메모리 기반의 방법이다
    - 새로운 샘플이 주어질 때마다 확률밀도함수($P_h$)를 다시 계산해야 하며, $\theta(nd)$만큼의 계산 시간이 소요된다 ($d$는 데이터의 차원 수, $n$은 샘플의 수)
    - 특징 공간이 고차원일수록 데이터가 희소한 문제가 생기므로, 데이터의 차원이 낮은 경우에만 사용하는 방법이다
- 가우시안 혼합
  - 데이터가 일정한 모양의 분포(Gaussian distribution 등)를 따른다는 가정하에 확률분포를 추정하는 방법
    - $P(x) = N(x;\mu, \Sigma) = \frac{1}{\sqrt{|\Sigma|}\sqrt{(2\pi)^d}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))$
    - $\mu = \frac{1}{n}\displaystyle\sum_{i=1,n}{x_i}$이고, $\Sigma=\frac{1}{n}\displaystyle\sum_{i=1,n}{(x_i-\mu)(x_i-\mu)^T}$
  - 가우시안을 이용하는 방법은 몇 개의 매개변수로 확률분포를 정의하는 모수적 방법(parametric method)이다 
  - 가우시안을 활용하면 평균 벡터 $\mu$와 공분산 행렬 $\Sigma$를 계산하면 훈련집합이 없어도 확률분포를 계산할 수 있다
  - 데이터가 $d$차원이라면 $\mu$는 $d$개, $\Sigma$는 $d^2$개의 요소를 가지므로, 총 $d+d^2$개의 값만 저장하면 된다
  - 확률밀도함수 $P(x)$는 가우시안 $k$개의 선형 결합으로 표현할 수 있다
    - $P(x) = \displaystyle\sum_{j=1}^k{\pi_j N(x; \mu_j, \Sigma_j)}
    - 혼합 계수 $\pi_j$는 0과 1사이의 값이고, $\displaystyle\sum_{j=1}^k{\pi_j} = 1$을 만족하며, 가우시안의 갯수 $k$는 사용자가 설정한다고 가정한다
    - 추정해야 할 매개변수집합: $(\pi_1, \pi_2, \cdots, \pi_k), (\mu_1, \Sigma_1), \cdots, (\mu_k, \Sigma_k)$
  - 최대 우도를 이용한 최적화
    - $P(X|\Theta) = \displaystyle\prod_{i=1}^n{P(x_i|\Theta)} = \displaystyle\prod_{i=1}^n{(\displaystyle\sum_{j=1}^k{\pi_j N(x; \mu_j, \Sigma_j)})}$
    - $\log{P(X|\Theta)} = \displaystyle\sum_{i=1}^n{\log{(\displaystyle\sum_{j=1}^k{\pi_j N(x; \mu_j, \Sigma_j)})}}$
    - 주어진 $X$가 발생할 가능성이 가장 큰 $\Theta$를 찾기 위해 EM 알고리즘을 사용
      - 가우시안이 자신에 속하는 샘플을 알면, 소속된 샘플을 이용하여 $\mu$와 $\Sigma$를 계산하고, 계산된 매개변수로 가우시안에 소속된 샘플의 정보를 개선할 수 있다
      - 샘플 $x_i$가 $j$번째 가우시안에 속할 확률을 $z_{ji}$라 하면, 이 확률 정보를 $k*n$(가우시안 개수 x 훈련집합 수) 크기의 행렬 $Z$에 저장할 수 있다
      - E단계: $z_{ji} = \frac{\pi_j N(x_i)}{\displaystyle\sum_{q=1}^k{pi_q N(x_i)}}$
      - M단계
        - $\mu_j = \frac{1}{n_j}\displaystyle\sum_{i=1}^n{z_{ji}x_i}$, $n_j = \displaystyle\sum_{i=1}^n{z_{ji}}$
        - $\Sigma_j = \frac{1}{n_j}\displaystyle\sum_{i=1}^n{z_{ji}(x_i-\mu_j)(x_i-\mu_j)^T}$, $pi_j = \frac{n_j}{n}$


### 선형 인자 모델

















