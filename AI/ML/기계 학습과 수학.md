# 기계 학습과 수학

### 벡터와 행렬
- 기계 학습에서는 입력된 샘플을 특징 벡터로 표현한다
  - ![image](https://github.com/kimho1wq/TIL/assets/15611500/bdd5ea8a-7592-4a19-b3f1-2c7d448f4c62)
  - 벡터(vector) $x$를 4차원이라고 한다면 수학적으로 $x \in R^4$ 라고 표현한다(x는 4차원 실수 공간의 한 점)
  - 행렬(matrix) $X$는 여러 개의 벡터를 담고, 가로(row)와 세로(column) 150x4 크기의 행렬은 $x \in R^{150*4}$ 라고 표현한다
    - ![image](https://github.com/kimho1wq/TIL/assets/15611500/82ab4706-74f4-4c33-8a7b-1421bfd53432)
  - $d$차원 벡터는 $d*1$ 행렬로 간주할 수 있으며, 기계 학습에서 훈련집합을 담은 행렬을 설계 행렬(design matrix)라고 한다
  - $A^{T}$ 는 $A$ 행렬의 전치(transpose)라고 하며, $A$의 행과 열을 교환한 형태이다
  - 차원이 같은 두 벡터 $a$와 $b$의 곱을 내적(dot product)라고 한다
    - ![image](https://github.com/kimho1wq/TIL/assets/15611500/3bb3550e-77cc-4bcb-899e-f0dc0756e2bb)
  - 모든 차원을 포괄하는 표현으로 텐서(tensor)라는 용어를 사용하며, 스칼라는 0차원, 벡터는 1차원, 행렬은 2차원 텐서에 해당한다
- 놈(norm)과 유사도(similarity)
  - 벡터의 크기를 정의할 때는 놈(norm)을 사용하며, p차 norm을 Lp놈이라 한다
    - ![image](https://github.com/kimho1wq/TIL/assets/15611500/8972e870-2968-48c8-baa0-f6d382a7c8c8)
  - 2차 놈( $||x||_{2} = (x_{1}^2 + x_{2}^2 + ... + x_{d}^2)^{1/2}$ )을 유클리디언 놈(Euclidean norm) 또는 L2놈이라 하고, 가장 많이 쓰이며 종종 첨자를 생략하고 표기한다
  - 길이가 1인 벡터를 단위 벡터(unit vector)라고 부르며, L2놈으로 벡터를 나눠서 만든다( $\frac{x}{||x||_{2}}$ )
    - ![image](https://github.com/kimho1wq/TIL/assets/15611500/e30ce6dc-c862-4a68-915b-83e146b60296)
  - 프로베니우스 놈(Frobenius norm)은 요소들의 제곱합의 제곱근이다
    - ![image](https://github.com/kimho1wq/TIL/assets/15611500/87a417ef-b8c2-43c5-a4ff-a31db77a7d7c)
  - 코사인 유사도(cosine similarity): 벡터를 단위 벡터로 만들어 내적을 이용하여 유사도를 측정하는 방법
    - ![image](https://github.com/kimho1wq/TIL/assets/15611500/4176e37c-72c5-48fc-b0ff-dca88f417838)
- 퍼셉트론(perceptron)
  - 입력 샘플을 2개의 부루 중 하나로 분류하는 분류기(classifier)이다
    - ![image](https://github.com/kimho1wq/TIL/assets/15611500/a0db3b83-6c99-42e9-abd6-236b9d467745)
    - ![image](https://github.com/kimho1wq/TIL/assets/15611500/b58a1988-8aa8-490e-8c1e-a82381ade70a)
    - d차원의 특징 벡터 x를 입력하여 해당 wedge의 가중치를 곱하여 더한값을 활성값(activation value)라고 한다
    - 활성값을 활성함수(activation function)에 넣었을 때 출력되는 값이 output이 된다
    - 전체 공간을 분할하는 결정 직선(decision line)은 가중치(w)에 수직이고, 원점으로부터 $\frac{T}{||w||_{2}}$
  - 가중치 벡터($w$)를 각 부류의 기준 샘플이라고 생각한다면, 신경망은 입력된 특징 벡터 $x$에 대해 $c$개의 부류와 유사도(벡터의 내적)를 계산하는 것이다
    - ![image](https://github.com/kimho1wq/TIL/assets/15611500/0ca1774b-b708-49b7-8c1d-5170e089fb59)
- 선형결합과 벡터공간
  - 벡터 $a$와 $b$에 각 상수를 곱한 후 더하는 연산($c = \alpha_{1}a + \alpha_{2}b$)을 선형결합(linear combination)이라고 한다 
  - 선형결합으로 만들어지는 공간을 벡터공간(vector space)라고 하고, 벡터공간을 만드는 벡터들을 기저 벡터(basis vector)라고 한다
  - 서로 수직인 기저 벡터를 정규 직교(orthonormal) 기저 벡터라고 하며, 한 기저 벡터를 나머지 기저 벡터의 선형결합으로 만들 수 없다면 선형독립(linear independence)하다고 한다
  - 행렬의 계수(rank)란 선형독립인 행의 갯수로 나타내고 계수가 n이면 벡터공간이 n차원이 되며, 행렬의 계수가 벡터의 차원과 같으면 행렬이 최대계수(full rank)를 가졌다고 말한다
- 역행렬
  - 정사각행렬 A에 대해 $A^{-1}A = AA^{-1} = I$를 만족하는 행렬 $A^{-1}$를 $A$의 역행렬(inverse matrix)라고 한다
  - 역행렬이 없는 행렬을 특이행렬(singular matrix)라고 하며, A가 역행렬을 가질때 다음 조건은 전부 필요충분조건이다
    - ```
      - A는 특이행렬이 아니다.
      - A는 최대계수를 가진다.
      - A의 모든 행이 선형독립이다.
      - A의 모든 열이 선형독립이다.
      - A의 행렬식은 0이 아니다.
      - A^(T)A는 양의 정부호(positive definite) 대칭 행렬이다.
      - A의 고윳값(eigen value)은 모두 0이 아니다.
      ```
  - A의 행렬식(determinant)은 $det(A)$로 표기하며, 크기가 4x4 이상인 행렬의 행렬식을 구하려면 가우스-요르단 소거법(Gauss-Hordan elimination)을 사용해야 한다
    - ![image](https://github.com/kimho1wq/TIL/assets/15611500/825448c6-ab6a-4f7f-8bd9-282203ae857c)
    - 행렬식은 기하학적으로 2차원에서는 2개의 행 벡터가 이루는 평행사변형의 넓이이고, 3차원에서는 3개의 행 벡터가 이루는 평행사각기둥의 부피이다
  - 행렬식을 이용하여 역행렬을 구할 수 있다
    - ![image](https://github.com/kimho1wq/TIL/assets/15611500/29d34948-246c-4686-9fff-c5dbf1b43c9d)
  - 정부호 행렬
    - 양의 정부호 행렬(positive definite) 행렬: 0이 아닌 모든 벡터 x에 대하여, $x^{T}Ax>0$
      - 양의 정부호 행렬의 고윳값은 모두 양수(>0)이며 모든 벡터를 양수로 만든다
    - 양의 준정부호(positive semi-definite) 행렬: 0이 아닌 모든 벡터 x에 대하여, $x^{T}Ax>=0$
    - 음의 정부호(negative definite) 행렬: 0이 아닌 모든 벡터 x에 대하여, $x^{T}Ax<0$
    - 음의 준정부호(negative semi-definite) 행렬: 0이 아닌 모든 벡터 x에 대하여, $x^{T}Ax<=0$
    - 부정부호 행렬(indefinite matrix): 벡터에 따라 음수가 되기도 하고 양수가 되기도 한다
- 고유 벡터(eigen vector)와 고유값(eigen value)
  - $Av = \lambda v$를 만족하는 0이 아닌 벡터 $v$를 행렬 $A$의 고유 벡터(eigen vector)라고 하며, 실수 $\lambda$를 고윳값(eigen value)라고 한다
  - m*m 행렬은 최대 m개의 고윳값과 고유 벡터를 가질 수 있으며, 모든 고유 벡터를 서로 직교(orthogonal) 한다(벡터의 내적 = 0)
  - 고유값 분해(eigen value decomposition)
    - 행렬 $A$를 $A = QA^{'}Q^{-1}$ 로 분해할 수 있다
    - $Q$는 $A$의 고유 벡터(eigen vector)를 열에 배치한 행렬이고, $A^{'}$는 고유값(eigen value)를 대각선에 배치한 대각행렬이다
    - ![image](https://github.com/kimho1wq/TIL/assets/15611500/8e0d7111-83e1-4b4b-adfb-a8dd38095b7b)
  - A가 정사각행렬에서 $det(A-\lambda I)=0$을 만족하는 방정식을 풀어서 고유값(eigen value)를 구할 수 있다 


### 확률변수와 확률분포
  - 시행의 결과에 따라 값이 결정되는 변수를 확률변수(random variable)라고 하고, 확률변수가 가질 수 있는 값의 집합을 정의역이라고 한다
  - 정의역 $x \in 1,2,...$ 전체에 결체 확률을 표현한 것을 확률분포(probability distribution)라고 하며 $P(x=1)$로 표현할 수 있다
  - 정의역이 이산일 때의 확률분포를 확률질향함수(probability mass function)이라 하고, 연속일 때는 확률밀도함수(probability density function)라고 한다
- 곱 규칙과 합 규칙
  - 결합 확률(joint probability): 두 사건이 결합된 상태의 확률, $P(x=하양, y=1)$
  - 조건부 확률(conditional probability): 사건이 이미 발생한 조건에서 다른 사건이 발생할 확률, $P(x=하양|y=1)$
  - 곱 규칙(production rule): $P(y,z) = P(x|y)P(y)$
  - 합 규칙(sum rule): $\displaystyle\sum_{y}^{}{P(y,z)}$ = $\displaystyle\sum_{y}^{}{P(x|y)P(y)}$
  - 독립(independent): $P(x,y) = P(x)P(y)$를 만족한다면 두 확률변수가 독립이라고 할 수 있다
  - 조건부 독립(conditional independent): 3개의 확률변수가 있을 때, $P(x,y|z) = P(x|z)P(y|z)$ 이면 x와 y는 z가 주어졌을 때 조건부 독립이라고 한다
- 기계 학습에 확률 적용
  - 사후확률(posterior probability): $P(y|x)$, 사건이 벌어져서 x를 관찰한 후 어느 y에서 발생했는지 나타내는 확률
  - 사전확률(prior probability): $P(y)$, 사건 x와 무관하게 미리 알 수 있는 확률
  - 우도(likelibood): $P(x|y)$, x는 알고 있는 사건이고 y는 추정해야 하는 사건으로 역 확률 문제( $L(y|x)$ )라고도 함
  - 베이즈 정리(Bayes formula): x,y의 결합확률이나 y,x가 일어날 결합확률이 같다
    - $P(y,x)=P(x|y)P(y) = P(x,y)=P(y|x)P(x)$
    - $P(y|x) = \frac{P(x|y)P(y)}{P(x)}$
  - ```문제) 새로운 샘플 x가 입력되었을 때, x를 y의 부류 중 하나로 분류해야 한다```
    - x가 어떤 부류에 속할 확률인 사후확률( $P(1|x), P(2|x), P(3|x)$ )을 구한 다음 가장 큰 값을 가지는 부류( $\hat{y} = argmax_{y}P(y|x)$ )로 분류해야 한다
    - 단, x가 만드는 공간은 굉장히 큰 차원으로 모든 확률을 표현할 수 없기 때문에, 베이즈 공식의 사전확률( $P(y)$ ), 우도( $P(x|y)$ )를 이용하여 계산한다
    - 무작위로 n개의 x를 추출한 후, 그 중에서 y의 비율로 사전확률( $P(y)$ )을 구할 수 있다
    - 우도( $P(x|y)$ )는 부류 y가 고정되어 있어서 다른 부류의 샘플을 배제한 후, y에 속하는 샘플만 가지고 확률 분포를 측정할 수 있다
- 평균과 분산
  - 평균(mean): $\mu = \frac{1}{n}\displaystyle\sum_{i=1}^{n}{x_{i}}$
  - 분산(variance): $V = \frac{1}{n}\displaystyle\sum_{i=1}^{n}{(x_{i}-\mu)^2}$
  - 표준편차(standard deviation): $V = \sigma^2 $
  - 특징이 d개인 데이터의 확률 벡터를 $x = (x_{1},\cdots,x_{d})^{T}$라고 한다면
    - 평균 벡터: $\mu = (\mu_{1},\cdots,\mu_{d})^{T} = \frac{1}{n}\displaystyle\sum_{i=1}^{n}{x_{i}}$
    - 공분산 행렬(covariance matrix): $\sum = \frac{1}{n}\displaystyle\sum_{i=1}^{n}{(x_{i}-\mu)(x_{i}-\mu)^{T}}$
      - $(x_{i}-\mu)$ 는 d\*1 행렬 이므로 $(x_{i}-\mu)(x_{i}-\mu)^{T}$ 는 d\*d 행렬이 된다
      - 공분산 행렬의 요소 $\sigma_{ij}$는 i번째 특징과 j번째 특징의 상호 변화 양상을 표현한다
      - 양수라면 i가 커질 때 j도 커지고, 음수라면 i가 커질 때 j는 작아진다
- 유용한 확률분포
  - 가우시안 분포(Gaussian distribution)
    - $N(x;\mu, \sigma^2) = \frac{1}{\sigma\sqrt{2\pi}}exp(-\frac{1}{2}(\frac{x-\mu}{\sigma}))^2$
    - 정규분포(normal distribution) 라고도 하며 최댓값을 가지는 지점은 $\mu$이고, $\sigma^2$는 분포의 퍼진 정도를 나타내며 값이 클수록 봉우리가 낮고 좌우로 멀리 퍼진다
  - 베르누이 분포(Bemoulli distribution)
    - $Ber(x;p) = p^x(1-p)^{1-x}$ 
    - 확률변수 x가 1(성공) 또는 0(실패)의 두 가지 값만 가질 수 있는 이진 변수이고, 성공 확률은 p이며 실패 확률은 1-p이다
  - 이항 분포(binomial distribution)
    - $B(x;n,p) = \binom{n}{x}p^x(1-p)^{n-x} = \frac{n!}{x!(n-x)!}p^x(1-p)^{n-x}$
    - 성공 확률이 p인 베르누이 실험을 n번 수행할 때 성공할 회수의 확률분포를 의미한다
- 정보이론
  - 정보이론(information theory)는 메시지의 정보량을 확률로 측정하며, 확률이 낮은 사건일수록 더 많은 정보를 전달한다고 할 수 있다
  - 자기 정보(self-information): 특정 사건 $e_{i}$의 정보량( $h(e_{i})$ )을 정보이론을 바탕으로 측정하는 방법 
    - $h(e_{i}) = -\log_{2} P(e_{i})$ 또는 $-\log_{e} P(e_{i})$
    - $-\log_{2} P(e_{i})$의 자기 정보 단위는 비트(bit)이고, 확률이 $\frac{1}{2}$일 때 1만큼의 정보량을 가진다
    - $-\log_{e} P(e_{i})$의 자기 정보 단위는 나츠(nat)이고, 확률이 $\frac{1}{e}$일 때 1만큼의 정보량을 가진다
  - 엔트로피(entropy): 확률분포의 무질서도 또는 불확실성(uncertainty)을 측정한다
    - $H(x)=-\displaystyle\sum_{i=1,k}{P(e_{i})\log_{2}{P(e_{i})}}$
    - 모든 사건의 확률이 동일할 수록 어떤 사건이 일어날지 예측하기가 어렵기 때문에 엔트로피가 높아지고 (ex, 윳놀이의 엔트로피 < 주사위의 엔트로피), 정의역의 크기가 커지면 엔트로피도 커진다
  - 교차 엔트로피(cross entropy): 두 확률분포 간의 엔트로피를 측정하기 위한 방법
    - $H(P,Q) = -\displaystyle\sum_{x}{P(x)\log_{2}{Q(x)}} = -\displaystyle\sum_{i=1,k}{P(e_{i})\log_{2}{Q(e_{i})}}$
    - $H(P,Q) = -\displaystyle\sum_{x}{P(x)\log_{2}{Q(x)}}$
      - $=-\displaystyle\sum_{x}{P(x)\log_{2}{P(x)}} +\displaystyle\sum_{x}{P(x)\log_{2}{P(x)}} -\displaystyle\sum_{x}{P(x)\log_{2}{Q(x)}}$
      - $=-\displaystyle\sum_{x}{P(x)\log_{2}{P(x)}} +\displaystyle\sum_{x}{P(x)\log_{2}{\frac{P(x)}{Q(x)}}}$
      - $=H(P) + \displaystyle\sum_{x}{P(x)\log_{2}{\frac{P(x)}{Q(x)}}}$
      - 마지막 식의 두 번째 항을 KL 다이버전스(divergence)라고 한다 (단, P=Q일때 0이 되어서 거리 개념을 내포하지만 KL(P||Q) != KL(Q||P) 이므로 거리는 아니다)
    - ```P와 Q의 교차 엔트로피 H(P,Q) = P의 엔트로피 + P와 Q간의 KL 다이버전스(divergence)```


### 최적화
- 보통 훈련집합은 아주 높은 차원에 정의된 매우 희소한 데이터이기 때문에 사후 확률분포를 이용하여 분류를 정확히 할 수 없다
- 따라서 기계학습에서는 적절한 모델과 목적 함수를 정의하고, 목적함수가 최저가 되는 극점(extreme point) 또는 최적해(optimal solution)을 찾아 모델의 매개 변수 공간을 탐색한다
  - 특징 공간에서 해야 하는 일을 모델의 매개변수 공간에서 하는 일로 바꾸는 과정이다
  - 다만, 일반적으로 모델의 매개변수 공간은 특징 공간보다 매우 넒다 (ex, MNIST의 특징공간은 28*28비트맵=784차원, 매개변수 공간은 거의 수백만차원)
- 미분에 의한 최적화
  - 편미분(partial differentiation)을 이용하여 다차원 공간에서의 최저점을 찾을 수 있으며, 편미분의 결과는 벡터 형태가 되는데, 이 벡터를 기울기(gradient)라고 한다
    - $\nabla f = f^{\prime} = \frac{\partial f}{\partial x} = (\frac{\partial f}{\partial x_{1}},\frac{\partial f}{\partial x_{2}})^{T}$
    - $-f^{\prime}(x)$ 방향으로 $x$를 움직이면 최저점을 찾을 수 있다
  - 연쇄 법칙(chain rule)
    - $f^{\prime}(x) = g^{\prime}(h(x))h^{\prime}(x)$
    - $f^{\prime}(x) = g^{\prime}(h(i(x)))h^{\prime}(i(x))i^{\prime}(x)$
- 야코비안 행렬(Jaconian matrix): d차원 벡터 x를 입력받아 m차원 벡터를 출력하는 m*d 행렬( $f:R^d \Rightarrow R^m$ ) 

$$J = \begin{pmatrix}
  \frac{\partial f_{1}}{\partial x_{1}} & \frac{\partial f_{1}}{\partial x_{2}} & \cdots & \frac{\partial f_{1}}{\partial x_{d}} \\
  \frac{\partial f_{2}}{\partial x_{1}} & \frac{\partial f_{2}}{\partial x_{2}} & \cdots & \frac{\partial f_{2}}{\partial x_{d}} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  \frac{\partial f_{m}}{\partial x_{1}} & \frac{\partial f_{m}}{\partial x_{2}} & \cdots & \frac{\partial f_{m}}{\partial x_{d}} 
\end{pmatrix}$$

- 헤시안 행렬(Hessian matrix): 함수 f를 $x_{ㅓ}$로 편미분한 후 $x_{i}$로 다시 편미분한 2차 편도함수의 n*n 대칭 행렬
 
$$H = \begin{pmatrix}
  \frac{\partial^2 f}{\partial x_{1}x_{1}} & \frac{\partial^2 f}{\partial x_{1}x_{2}} & \cdots & \frac{\partial^2 f}{\partial x_{1}x_{n}} \\
  \frac{\partial^2 f}{\partial x_{2}x_{1}} & \frac{\partial^2 f}{\partial x_{2}x_{2}} & \cdots & \frac{\partial^2 f}{\partial x_{2}x_{n}} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  \frac{\partial^2 f}{\partial x_{n}x_{1}} & \frac{\partial^2 f}{\partial x_{n}x_{2}} & \cdots & \frac{\partial^2 f}{\partial x_{n}x_{n}} \\
\end{pmatrix}$$

- 테일러 급수(Taylor series): 어떤 점 x에서의 함수값 $f(x)$와 미분 $f^{\prime}(x)$가 주어졌을 때 이웃한 점에서의 함수값을 추정하는 방법
  - $f(x+\Delta) = f(x) + \frac{f^{\prime}(x)}{1!}\Delta + \frac{f^{\prime\prime}(x)}{2!}\Delta^2 + \cdots + \frac{f^{(k)}(x)}{k!}\Delta^k + \cdots \approx f(x) + f^{\prime}(x)\Delta$
  - $x+\Delta$는 $x$로 부터 $\Delta$ 만큼 떨어져 있는 이웃점이다 ( $\Delta$가 작을수록 추정값이 더 정확함)
  









