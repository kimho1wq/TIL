# 다층 퍼셉트론

### 퍼셉트론(perceptron)
- 원시적인 신경망으로 노드, 가중치, 입력층, 출력층과 같은 구조로 되어있다
  - ![image](https://github.com/kimho1wq/TIL/assets/15611500/4e4931ac-2b6a-435f-b6df-38c09c0cd51e)
- 특징 벡터 $x = (x_{1}, \cdots , x_{d})^T$가 들어 오면, 서로 연결된 특징값과 가중치를 곱한 결과를 모두 더한 값 $s$를 활성함수(activation function)에 입력으로 넣고 계산한다
- 행렬 표기 
  - $s = \displaystyle\sum_{i=1}^{d}{w_i x_i + w_0} = w^T x + w_0 = w \cdot x + w_0$, ( $x = (x_1, \cdots, x_d)^T, w=(w_1, \cdots, w_d)^T$ )
  - $s = w^T x = w \cdot x$, ( $x = (1, x_1, \cdots, x_d)^T, w=(w_0, w_1, \cdots, w_d)^T$ )
- 단, 퍼셉트론은 층이 1개이기 때문에 선형분리(linearly separable)한 상황밖에 처리하지 못한다 (XOR 분류 문제등은 다층 퍼셉트론이 필요)
  - ![image](https://github.com/kimho1wq/TIL/assets/15611500/59d0fd73-3e4f-4ad0-afbf-04c389df535a)

### 다층 퍼셉트론(MLP, multi-layer perceptron)
- 여러 개의 퍼셉트론을 결합한 다층 구조를 이용하여 선형분리가 불가능(linearly non-separable)한 상황을 해결할 수 있다
  1. 퍼셉트론에 은닉층(hidden layer)을 두어서 원래 특징 공간을 새로운 특징 공간으로 변환시킨다
  2. 시그모이드(sigmoid) 등의 활성함수(activation function)을 도입한다
  3. 오류 역전파(back propagation) 알고리즘을 사용하여 한 층씩 기울기(gradient)를 계산하고 가중치를 갱신해 나간다
- 활성함수
  - ![image](https://github.com/kimho1wq/TIL/assets/15611500/8e3d8fce-c4b3-470c-9036-86efd61e1ffb)
  - 매개변수 $a$에 따라 기울기가 결정되며 $a$가 클수록 가파르게 된다
  - Non-linear activation function의 필요성
    - linear activation function을 사용한다면 multi-layer perceptron은 모두 single layer perceptron으로 바꿀 수 있게된다
    - ![image](https://github.com/kimho1wq/TIL/assets/15611500/7a9fd2bf-2845-4802-b9ec-c3ba68edfc9e)
- 은닉층(hidden layer): 원래 특징 공간을 새로운 특징 공간으로 변환하는 특징 추출기(feature extractor)로 볼 수 있다
- 병렬분산 구조
  - 다층 퍼셉트론은 분산처리와 병렬처리를 수행하므로 병렬분산 구조라고 한다
  - i번째 입력 노드인 $x_i$는 은닉층의 p개의 노드 모두에 영향을 미치고, 출력층 노드에 모두 영향을 미치기 때문에 분산처리를 한다고 말한다
  - 은닉층에 있는 한 노드가 t라는 시간을 소비하여 계산한다면, 각 노드는 GPU를 이용하여 각자 병렬적(독립적)으로 자신의 값을 계산하여 1개의 layer에서 t라는 시간만 소비할 수 있다
- 동작
  - l-1번째 은닉층의 i번째 노드와 j번째 노드를 연결하는 가중치를 $u^l_{ji}$로 표기 (0번째 은닉층은 입력층)
  - p은 은닉층 개수인 하이퍼 파라미터(hyper parameter), c는 출력 노드의 개수(부류의 개수), $\tau$는 활성함수(activation function)
  - 1번째 은닉층의 j번째 은닉 노드의 연산
    - $z_j = \tau(zsum_j)$, $j=1,2,\cdots,p$ 이때, $zsum_j = u^1_j x$ 이고 $u^1_j = (u^1_{j0},\cdots,u^1_{jd}), x=(1,x_1,\cdots,x_d)^T$
  - 출력층의 k번째 출력 노드의 연산
    - $o_k = \tau(osum_k)$, $k=1,2,\cdots,c$ 이때, $osum_k = u^2_k z$ 이고 $u^2_k = (u^2_{k0},\cdots,u^2_{kp}), z=(1,z_1,\cdots,z_p)^T$
  - 행렬로 표현: $o = \tau(U^2\tau_h(U^1x))$ 

### 오류 역전파 알고리즘
- 오류 역전파 알고리즘의 유도
  - 목적함수(J)를 평균제곱 오차(MSE, mean squared error)로 정한다면 $J(\theta) = \frac{1}{2}||y-o(\theta)||_2^2$ 로 표현할 수 있다
  - 경사 하강법을 이용하여 학습을 위해서는 각각의 가중치(u)에 대하여 미분한 값을 구해야 한다
  - 출력층의 한개의 노드에 영향을 받는 $u^2_{kj}$ 계산
    - $\frac{\partial J}{\partial u^2_{kj}} = \frac{\partial(\frac{1}{2}(y_k-o_k)^2)}{\partial u^2_{kj}}$
      - $= -(y_k - o_k)\frac{\partial o_k}{\partial u^2_{kj}}$
      - $= -(y_k - o_k)\frac{\partial \tau(osum_k)}{\partial u^2_{kj}}$
      - $= -(y_k - o_k)\tau^\prime(osum_k)\frac{\partial osum_k}{\partial u^2_{kj}}$
      - $= -(y_k - o_k)\tau^\prime(osum_k)z_j$
    - $\delta_k = (y_k - o_k) \tau^\prime(osum_k)$, $1 <= k <= c$
    - $\frac{\partial J}{\partial u^2_{kj}} = \-\delta_k z_j$, $0 <= j <= p, 1 <= k <= c$
  - 은닉층의 한개의 노드와 모든 출력층의 영향을 받는 $u^1_{ji}$ 계산
    - $\frac{\partial J}{\partial u^1_{ji}} = \frac{\partial(\frac{1}{2}\displaystyle\sum_{q=1}^c{(y_q-o_q)^2})}{\partial u^1_{ji}}$
      - $= -\displaystyle\sum_{q=1}^c{(y_q - o_q)} \frac{\partial o_q}{\partial u^1_{ji}}$
      - $= -\displaystyle\sum_{q=1}^c{(y_q - o_q)} \tau^\prime(osum_q) \frac{\partial osum_q}{\partial u^1_{ji}}$
      - $= -\displaystyle\sum_{q=1}^c{(y_q - o_q)} \tau^\prime(osum_q) \frac{\partial osum_q}{\partial z_j} \frac{\partial o_j}{\partial u^1_{ji}}$
      - $= -\displaystyle\sum_{q=1}^c{(y_q - o_q)} \tau^\prime(osum_q) u^2_{qj} \frac{\partial o_j}{\partial u^1_{ji}}$
      - $= -\displaystyle\sum_{q=1}^c{(y_q - o_q)} \tau^\prime(osum_q) u^2_{qj} \frac{\partial o_j}{\partial osum_j} \frac{\partial osum_j}{\partial u^1_{ji}}$
      - $= -\displaystyle\sum_{q=1}^c{(y_q - o_q)} \tau^\prime(osum_q) u^2_{qj} \tau^\prime(osum_j)x_j$
    - $\delta_j = \tau^\prime(osum_j) \displaystyle\sum_{q=1}^c{\delta_q u^2_{qj}}$, $1 <= j <= p$
    - $\frac{\partial J}{\partial u^1_{ji}} = \-\delta_j x_i$, $0 <= i <= d, 1 <= j <= p$
  - ![image](https://github.com/kimho1wq/TIL/assets/15611500/40dfc2a9-4da7-4b7f-bdff-2bc8464eb2de)

















